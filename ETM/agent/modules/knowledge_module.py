"""
Knowledge Representation Module

Multi-modal knowledge representation, including topic space, semantic vectors, and symbolic knowledge.
This module is responsible for knowledge storage, retrieval, and organization.
"""

import os
import sys
import json
import numpy as np
import faiss
from typing import List, Dict, Tuple, Optional, Union, Any
from pathlib import Path

# Add project root directory to path
sys.path.append(str(Path(__file__).parents[2]))

# Import other modules
from agent.modules.topic_aware import TopicAwareModule
from embedding.embedder import QwenEmbedder


class KnowledgeModule:
    """
    Knowledge representation module responsible for knowledge storage, retrieval, and organization.
    
    Features:
    1. Topic space: Topic distribution (theta) generated by ETM
    2. Semantic vectors: Document embeddings generated by Qwen
    3. Symbolic knowledge: Structured knowledge and rules
    """
    
    def __init__(
        self,
        topic_module: TopicAwareModule,
        embedder: QwenEmbedder,
        vector_dim: int = 1024,
        use_faiss: bool = True,
        dev_mode: bool = False
    ):
        """
        Initialize knowledge representation module.
        
        Args:
            topic_module: Topic-aware module
            embedder: Embedding model
            vector_dim: Vector dimension
            use_faiss: Whether to use FAISS for vector retrieval
            dev_mode: Whether to enable development mode (print debug information)
        """
        self.topic_module = topic_module
        self.embedder = embedder
        self.vector_dim = vector_dim
        self.use_faiss = use_faiss
        self.dev_mode = dev_mode
        
        # Document storage
        self.documents = []
        self.document_embeddings = []
        self.document_topic_dists = []
        
        # Initialize vector index
        if use_faiss:
            self.index = faiss.IndexFlatIP(vector_dim)  # Inner product similarity (cosine similarity)
        else:
            self.index = None
        
        if self.dev_mode:
            print(f"[KnowledgeModule] Initialized successfully")
    
    def add_document(
        self,
        document: Dict[str, Any],
        embedding: Optional[np.ndarray] = None,
        topic_dist: Optional[np.ndarray] = None
    ) -> int:
        """
        Add document to knowledge base.
        
        Args:
            document: Document content
            embedding: Document embedding vector (optional)
            topic_dist: Document topic distribution (optional)
            
        Returns:
            Document ID
        """
        # Generate embedding (if not provided)
        if embedding is None and "text" in document:
            embedding = self.embedder.embed_text(document["text"])
        
        # Generate topic distribution (if not provided)
        if topic_dist is None and embedding is not None:
            topic_dist = self.topic_module.get_topic_distribution(
                document["text"] if "text" in document else ""
            )
        
        # Add to storage
        doc_id = len(self.documents)
        self.documents.append(document)
        
        if embedding is not None:
            self.document_embeddings.append(embedding)
            
            # Add to FAISS index
            if self.use_faiss:
                # Normalize vector
                norm_embedding = embedding / np.linalg.norm(embedding)
                self.index.add(np.array([norm_embedding], dtype=np.float32))
        
        if topic_dist is not None:
            self.document_topic_dists.append(topic_dist)
        
        return doc_id
    
    def add_documents(
        self,
        documents: List[Dict[str, Any]],
        embeddings: Optional[List[np.ndarray]] = None,
        topic_dists: Optional[List[np.ndarray]] = None
    ) -> List[int]:
        """
        Batch add documents to knowledge base.
        
        Args:
            documents: List of documents
            embeddings: List of embedding vectors (optional)
            topic_dists: List of topic distributions (optional)
            
        Returns:
            List of document IDs
        """
        doc_ids = []
        
        # Batch generate embeddings (if not provided)
        if embeddings is None:
            texts = [doc.get("text", "") for doc in documents]
            embeddings = self.embedder.embed_texts(texts)
        
        # Batch generate topic distributions (if not provided)
        if topic_dists is None:
            topic_dists = []
            for i, doc in enumerate(documents):
                topic_dist = self.topic_module.get_topic_distribution(
                    doc.get("text", "")
                )
                topic_dists.append(topic_dist)
        
        # Batch add to storage
        for i, doc in enumerate(documents):
            embedding = embeddings[i] if i < len(embeddings) else None
            topic_dist = topic_dists[i] if i < len(topic_dists) else None
            
            doc_id = self.add_document(doc, embedding, topic_dist)
            doc_ids.append(doc_id)
        
        return doc_ids
    
    def query_by_vector(
        self,
        query_vector: np.ndarray,
        top_k: int = 5
    ) -> List[Tuple[int, float, Dict[str, Any]]]:
        """
        Query related documents by vector.
        
        Args:
            query_vector: Query vector
            top_k: Number of documents to return
            
        Returns:
            List of document ID, similarity, and document content
        """
        if len(self.documents) == 0:
            return []
        
        if self.use_faiss and self.index.ntotal > 0:
            # Normalize query vector
            norm_query = query_vector / np.linalg.norm(query_vector)
            
            # Use FAISS to query
            scores, indices = self.index.search(
                np.array([norm_query], dtype=np.float32),
                min(top_k, len(self.documents))
            )
            
            # Build results
            results = []
            for i, idx in enumerate(indices[0]):
                if idx < len(self.documents):
                    results.append((
                        idx,
                        float(scores[0][i]),
                        self.documents[idx]
                    ))
            
            return results
        else:
            # Manually calculate similarity
            similarities = []
            
            for i, emb in enumerate(self.document_embeddings):
                # Calculate cosine similarity
                norm_query = query_vector / np.linalg.norm(query_vector)
                norm_emb = emb / np.linalg.norm(emb)
                similarity = np.dot(norm_query, norm_emb)
                
                similarities.append((i, similarity))
            
            # Sort and return top k
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            results = [
                (idx, sim, self.documents[idx])
                for idx, sim in similarities[:top_k]
            ]
            
            return results
    
    def query_by_text(
        self,
        query_text: str,
        top_k: int = 5
    ) -> List[Tuple[int, float, Dict[str, Any]]]:
        """
        Query related documents by text.
        
        Args:
            query_text: Query text
            top_k: Number of documents to return
            
        Returns:
            List of document ID, similarity, and document content
        """
        # Generate query vector
        query_vector = self.embedder.embed_text(query_text)
        
        # Use vector query
        return self.query_by_vector(query_vector, top_k)
    
    def query_by_topic(
        self,
        topic_dist: np.ndarray,
        top_k: int = 5
    ) -> List[Tuple[int, float, Dict[str, Any]]]:
        """
        Query related documents by topic distribution.
        
        Args:
            topic_dist: Topic distribution
            top_k: Number of documents to return
            
        Returns:
            List of document ID, similarity, and document content
        """
        if len(self.documents) == 0 or len(self.document_topic_dists) == 0:
            return []
        
        # Calculate topic similarity
        similarities = []
        
        for i, doc_topic in enumerate(self.document_topic_dists):
            # Calculate cosine similarity
            similarity = self.topic_module.get_topic_similarity(topic_dist, doc_topic)
            similarities.append((i, similarity))
        
        # Sort and return top k
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        results = [
            (idx, sim, self.documents[idx])
            for idx, sim in similarities[:top_k]
        ]
        
        return results
    
    def hybrid_query(
        self,
        query_text: str,
        topic_weight: float = 0.3,
        semantic_weight: float = 0.7,
        top_k: int = 5
    ) -> List[Tuple[int, float, Dict[str, Any]]]:
        """
        Hybrid query (combining topic and semantic).
        
        Args:
            query_text: Query text
            topic_weight: Topic similarity weight
            semantic_weight: Semantic similarity weight
            top_k: Number of documents to return
            
        Returns:
            List of document ID, similarity, and document content
        """
        if len(self.documents) == 0:
            return []
        
        # Generate query vector
        query_vector = self.embedder.embed_text(query_text)
        
        # Generate topic distribution
        topic_dist = self.topic_module.get_topic_distribution(query_text)
        
        # Calculate hybrid similarity
        hybrid_scores = {}
        
        # Semantic similarity
        semantic_results = self.query_by_vector(query_vector, len(self.documents))
        for idx, score, _ in semantic_results:
            hybrid_scores[idx] = semantic_weight * score
        
        # Topic similarity
        topic_results = self.query_by_topic(topic_dist, len(self.documents))
        for idx, score, _ in topic_results:
            if idx in hybrid_scores:
                hybrid_scores[idx] += topic_weight * score
            else:
                hybrid_scores[idx] = topic_weight * score
        
        # Sort and return top k
        sorted_scores = sorted(
            hybrid_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        results = [
            (idx, score, self.documents[idx])
            for idx, score in sorted_scores[:top_k]
        ]
        
        return results
    
    def save(self, path: str) -> None:
        """
        Save knowledge base to file.
        
        Args:
            path: Save path
        """
        # Create directory
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # Save documents
        data = {
            "documents": self.documents,
            "document_embeddings": [emb.tolist() for emb in self.document_embeddings],
            "document_topic_dists": [dist.tolist() for dist in self.document_topic_dists]
        }
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # If using FAISS, save index
        if self.use_faiss and self.index is not None:
            index_path = f"{path}.index"
            faiss.write_index(self.index, index_path)
    
    @classmethod
    def load(
        cls,
        path: str,
        topic_module: TopicAwareModule,
        embedder: QwenEmbedder,
        use_faiss: bool = True,
        dev_mode: bool = False
    ) -> 'KnowledgeModule':
        """
        Load knowledge base from file.
        
        Args:
            path: Load path
            topic_module: Topic-aware module
            embedder: Embedding model
            use_faiss: Whether to use FAISS
            dev_mode: Whether to enable development mode
            
        Returns:
            Knowledge representation module instance
        """
        # Create instance
        instance = cls(
            topic_module=topic_module,
            embedder=embedder,
            use_faiss=use_faiss,
            dev_mode=dev_mode
        )
        
        # Load data
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        instance.documents = data["documents"]
        instance.document_embeddings = [np.array(emb) for emb in data["document_embeddings"]]
        instance.document_topic_dists = [np.array(dist) for dist in data["document_topic_dists"]]
        
        # If using FAISS, load index
        if use_faiss:
            index_path = f"{path}.index"
            if os.path.exists(index_path):
                instance.index = faiss.read_index(index_path)
            else:
                # Rebuild index
                instance.index = faiss.IndexFlatIP(instance.vector_dim)
                
                for emb in instance.document_embeddings:
                    norm_emb = emb / np.linalg.norm(emb)
                    instance.index.add(np.array([norm_emb], dtype=np.float32))
        
        return instance


# Test code
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Test knowledge representation module")
    parser.add_argument("--etm_model", type=str, required=True, help="ETM model path")
    parser.add_argument("--vocab", type=str, required=True, help="Vocabulary path")
    parser.add_argument("--query", type=str, required=True, help="Test query")
    parser.add_argument("--dev_mode", action="store_true", help="Development mode")
    
    args = parser.parse_args()
    
    # Initialize components
    from agent.modules.topic_aware import TopicAwareModule
    
    topic_module = TopicAwareModule(
        etm_model_path=args.etm_model,
        vocab_path=args.vocab,
        dev_mode=args.dev_mode
    )
    
    embedder = QwenEmbedder()
    
    # Initialize knowledge representation module
    knowledge_module = KnowledgeModule(
        topic_module=topic_module,
        embedder=embedder,
        dev_mode=args.dev_mode
    )
    
    # Add test documents
    test_docs = [
        {"id": "doc1", "text": "Climate change is one of the biggest environmental challenges facing the world today."},
        {"id": "doc2", "text": "Renewable energy includes solar, wind, and hydroelectric power."},
        {"id": "doc3", "text": "Carbon emission trading is a market mechanism to reduce greenhouse gas emissions."},
        {"id": "doc4", "text": "Energy transition refers to the process of shifting from fossil fuels to clean energy."},
        {"id": "doc5", "text": "Coal is a non-renewable fossil fuel that releases carbon dioxide when burned."}
    ]
    
    knowledge_module.add_documents(test_docs)
    
    # Test query
    print(f"Query: {args.query}")
    
    # Semantic query
    semantic_results = knowledge_module.query_by_text(args.query)
    print("\nSemantic search results:")
    for idx, score, doc in semantic_results:
        print(f"  {doc['id']} (score: {score:.4f}): {doc['text']}")
    
    # Topic query
    topic_dist = topic_module.get_topic_distribution(args.query)
    topic_results = knowledge_module.query_by_topic(topic_dist)
    print("\nTopic search results:")
    for idx, score, doc in topic_results:
        print(f"  {doc['id']} (score: {score:.4f}): {doc['text']}")
    
    # Hybrid query
    hybrid_results = knowledge_module.hybrid_query(args.query)
    print("\nHybrid search results:")
    for idx, score, doc in hybrid_results:
        print(f"  {doc['id']} (score: {score:.4f}): {doc['text']}")

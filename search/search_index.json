{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"THETA Topic Model","text":"<p>Advanced Topic Modeling with Qwen Embeddings</p> <p>THETA is a state-of-the-art topic modeling framework that leverages Qwen3-Embedding models to achieve superior performance in topic discovery and analysis. Designed as an improvement over traditional topic models like LDA and ETM, THETA combines the power of large language model embeddings with advanced neural topic modeling architectures.</p> <ul> <li> <p> Getting Started</p> <p>Install THETA and train your first topic model in minutes</p> <p> Quick Start</p> </li> <li> <p> User Guide</p> <p>Complete workflow from data preparation to result analysis</p> <p> User Guide</p> </li> <li> <p> Models</p> <p>Architecture details of THETA and baseline models</p> <p> Models</p> </li> <li> <p> API Reference</p> <p>Complete parameter documentation for all CLI tools</p> <p> API Reference</p> </li> </ul>"},{"location":"#key-features","title":"Key Features","text":"Feature Description Powerful Embeddings Built on Qwen3-Embedding (0.6B / 4B / 8B) for superior semantic understanding Flexible Training Zero-shot, supervised, and unsupervised modes Rich Visualizations Topic distributions, heatmaps, UMAP projections, pyLDAvis Multilingual Full support for English and Chinese data Extensible Easy customization with new datasets and configurations Comprehensive Evaluation TD, TC, NPMI, and more metrics"},{"location":"#model-comparison","title":"Model Comparison","text":"Model Embedding Type Characteristics THETA Qwen3-Embedding Neural Our method \u2014 best performance LDA \u2014 Probabilistic Classic generative model ETM Word2Vec Neural Embedded topic model CTM SBERT Neural Contextualized model DTM SBERT Neural Dynamic temporal model"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># 1. Preprocess data\npython prepare_data.py \\\n    --dataset 20ng \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --gpu 0\n\n# 2. Train model\npython run_pipeline.py \\\n    --dataset 20ng \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --gpu 0\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If you use THETA in your research, please cite:</p> <pre><code>@article{theta2025,\n  title={THETA: Advanced Topic Modeling with Qwen Embeddings},\n  author={CodeSoul},\n  year={2025}\n}\n</code></pre>"},{"location":"#links","title":"Links","text":"<ul> <li> GitHub Repository</li> <li> Website</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions for THETA topic modeling.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#cuda-not-available","title":"CUDA Not Available","text":"<p>Problem: <pre><code>RuntimeError: CUDA is not available\ntorch.cuda.is_available() returns False\n</code></pre></p> <p>Solutions:</p> <p>Check CUDA installation: <pre><code>nvidia-smi\nnvcc --version\n</code></pre></p> <p>Reinstall PyTorch with CUDA support: <pre><code>pip uninstall torch\npip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p>"},{"location":"troubleshooting/#import-errors","title":"Import Errors","text":"<p>Problem: <pre><code>ModuleNotFoundError: No module named 'transformers'\n</code></pre></p> <p>Solution: <pre><code>pip install -r requirements.txt\n</code></pre></p>"},{"location":"troubleshooting/#version-conflicts","title":"Version Conflicts","text":"<p>Create fresh virtual environment: <pre><code>conda create -n theta_clean python=3.9\nconda activate theta_clean\npip install -r requirements.txt\n</code></pre></p>"},{"location":"troubleshooting/#model-download-failures","title":"Model Download Failures","text":"<p>Problem: <pre><code>OSError: Can't load model from 'Qwen/Qwen-Embedding-0.6B'\n</code></pre></p> <p>Solution: Download manually: <pre><code>git lfs install\ngit clone https://huggingface.co/Qwen/Qwen-Embedding-0.6B\nmv Qwen-Embedding-0.6B /root/embedding_models/qwen3_embedding_0.6B/\n</code></pre></p>"},{"location":"troubleshooting/#data-issues","title":"Data Issues","text":""},{"location":"troubleshooting/#file-not-found","title":"File Not Found","text":"<p>Problem: <pre><code>FileNotFoundError: /root/autodl-tmp/data/my_dataset/my_dataset_cleaned.csv\n</code></pre></p> <p>Solution: Verify naming convention <code>{dataset_name}_cleaned.csv</code>: <pre><code>mkdir -p /root/autodl-tmp/data/my_dataset\ncp your_file.csv /root/autodl-tmp/data/my_dataset/my_dataset_cleaned.csv\n</code></pre></p>"},{"location":"troubleshooting/#missing-required-columns","title":"Missing Required Columns","text":"<p>Problem: <pre><code>KeyError: 'text'\n</code></pre></p> <p>Solution: Rename column to standard name: <pre><code>import pandas as pd\ndf = pd.read_csv('data.csv')\ndf.rename(columns={'content': 'text'}, inplace=True)\ndf.to_csv('data_fixed.csv', index=False)\n</code></pre></p> <p>Accepted text column names: <code>text</code>, <code>content</code>, <code>cleaned_content</code>, <code>clean_text</code></p>"},{"location":"troubleshooting/#encoding-errors","title":"Encoding Errors","text":"<pre><code>iconv -f ISO-8859-1 -t UTF-8 input.csv &gt; output.csv\n</code></pre>"},{"location":"troubleshooting/#empty-or-invalid-data","title":"Empty or Invalid Data","text":"<p>Check data statistics: <pre><code>python -c \"\nimport pandas as pd\ndf = pd.read_csv('data.csv')\nprint(f'Rows: {len(df)}')\nprint(f'Empty text: {df[\\\"text\\\"].isna().sum()}')\nprint(f'Avg length: {df[\\\"text\\\"].str.len().mean():.1f}')\n\"\n</code></pre></p>"},{"location":"troubleshooting/#training-issues","title":"Training Issues","text":""},{"location":"troubleshooting/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Solutions:</p> <p>Reduce batch size: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --batch_size 16 \\\n    --gpu 0\n</code></pre></p> <p>Memory requirements by configuration:</p> Model Size Batch Size VRAM Required 0.6B 16 ~6GB 0.6B 32 ~8GB 0.6B 64 ~12GB 4B 8 ~10GB 4B 16 ~14GB 8B 8 ~18GB 8B 16 ~28GB"},{"location":"troubleshooting/#training-not-converging","title":"Training Not Converging","text":"<p>Solutions:</p> <p>Reduce learning rate: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --learning_rate 0.001 \\\n    --gpu 0\n</code></pre></p> <p>Adjust KL annealing: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --kl_start 0.0 \\\n    --kl_end 1.0 \\\n    --kl_warmup 80 \\\n    --gpu 0\n</code></pre></p>"},{"location":"troubleshooting/#early-stopping-too-soon","title":"Early Stopping Too Soon","text":"<p>Increase patience: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --patience 20 \\\n    --gpu 0\n</code></pre></p> <p>Or disable early stopping: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --epochs 200 \\\n    --no_early_stopping \\\n    --gpu 0\n</code></pre></p>"},{"location":"troubleshooting/#nan-or-inf-values","title":"NaN or Inf Values","text":"<p>Reduce learning rate significantly: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --learning_rate 0.0005 \\\n    --gpu 0\n</code></pre></p> <p>Check for data issues: <pre><code>python -c \"\nimport numpy as np\nembeddings = np.load('result/0.6B/my_dataset/bow/qwen_embeddings_zeroshot.npy')\nprint(f'Contains NaN: {np.isnan(embeddings).any()}')\nprint(f'Contains Inf: {np.isinf(embeddings).any()}')\n\"\n</code></pre></p>"},{"location":"troubleshooting/#evaluation-issues","title":"Evaluation Issues","text":""},{"location":"troubleshooting/#poor-metric-scores","title":"Poor Metric Scores","text":"<p>Solutions:</p> <ul> <li>Train longer with <code>--epochs 200 --no_early_stopping</code></li> <li>Adjust topic count: try 10, 15, 20, 25, 30</li> <li>Improve data quality: clean text more thoroughly, remove short documents</li> <li>Tune hyperparameters: <code>--hidden_dim 768 --learning_rate 0.001 --kl_warmup 80</code></li> </ul>"},{"location":"troubleshooting/#metric-computation-errors","title":"Metric Computation Errors","text":"<p>Minimum requirements: - Documents: 500+ - Average length: 20+ words - Vocabulary: 1000+ words</p>"},{"location":"troubleshooting/#visualization-issues","title":"Visualization Issues","text":""},{"location":"troubleshooting/#visualization-generation-fails","title":"Visualization Generation Fails","text":"<p>Install required fonts: <pre><code># Ubuntu/Debian\napt-get install fonts-liberation fonts-noto-cjk\n\n# macOS\nbrew install font-liberation font-noto-cjk\n</code></pre></p> <p>Set matplotlib backend: <pre><code>export MPLBACKEND=Agg\npython run_pipeline.py --dataset my_dataset --models theta\n</code></pre></p>"},{"location":"troubleshooting/#chinese-characters-not-displaying","title":"Chinese Characters Not Displaying","text":"<p>Install Chinese fonts: <pre><code>apt-get install fonts-noto-cjk fonts-wqy-zenhei\n</code></pre></p> <p>Specify language parameter: <pre><code>python run_pipeline.py \\\n    --dataset chinese_dataset \\\n    --models theta \\\n    --language zh \\\n    --gpu 0\n</code></pre></p>"},{"location":"troubleshooting/#low-resolution-images","title":"Low Resolution Images","text":"<p>Increase DPI: <pre><code>python -m visualization.run_visualization \\\n    --result_dir result/0.6B \\\n    --dataset my_dataset \\\n    --mode zero_shot \\\n    --model_size 0.6B \\\n    --dpi 600 \\\n    --language en\n</code></pre></p> <p>DPI recommendations: Screen=150, Document=300, Publication=600, Poster=1200</p>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-preprocessing","title":"Slow Preprocessing","text":"<p>Increase batch size: <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --batch_size 64 \\\n    --gpu 0\n</code></pre></p> <p>Monitor GPU utilization: <code>nvidia-smi dmon</code></p>"},{"location":"troubleshooting/#memory-leaks","title":"Memory Leaks","text":"<p>Clear cache periodically: <pre><code>import gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache()\n</code></pre></p>"},{"location":"troubleshooting/#specific-error-messages","title":"Specific Error Messages","text":"Error Solution \"Dataset directory does not exist\" <code>mkdir -p /root/autodl-tmp/data/my_dataset</code> \"Preprocessed files not found\" Run <code>prepare_data.py</code> first \"Model checkpoint not found\" Run training first \"Invalid number of topics\" Use range 5-100 \"Supervised mode requires labels\" Add label column or use <code>--mode zero_shot</code> \"DTM requires time column\" Add <code>--time_column year</code> to preprocessing"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#report-issues","title":"Report Issues","text":"<p>When reporting issues, include: 1. Complete error message 2. Command that produced error 3. System information (GPU, CUDA version) 4. Dataset characteristics (size, language)</p> <p>System information: <pre><code>python -c \"\nimport torch\nimport sys\nprint(f'Python: {sys.version}')\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA: {torch.version.cuda}')\nprint(f'GPU: {torch.cuda.get_device_name(0)}')\n\"\n</code></pre></p>"},{"location":"troubleshooting/#community-resources","title":"Community Resources","text":"<ul> <li>GitHub Issues: Report bugs</li> <li>GitHub Discussions: Ask questions</li> <li>Documentation: https://theta.code-soul.com</li> <li>Email: support@theta.code-soul.com</li> </ul>"},{"location":"advanced/chinese-data/","title":"Chinese Data Processing","text":"<p>Specialized guide for processing Chinese text with THETA.</p>"},{"location":"advanced/chinese-data/#specialized-preprocessing","title":"Specialized Preprocessing","text":"<p>Chinese text requires different handling than English:</p> <p>Data cleaning: <pre><code>python -m dataclean.main \\\n    --input /root/autodl-tmp/data/chinese_corpus/raw_data.csv \\\n    --output /root/autodl-tmp/data/chinese_corpus/chinese_corpus_cleaned.csv \\\n    --language chinese\n</code></pre></p> <p>Cleaning operations for Chinese: - Remove HTML entities - Normalize full-width and half-width characters - Handle Chinese punctuation - Preserve Chinese word boundaries - Convert traditional to simplified (optional)</p> <p>Preprocessing: <pre><code>python prepare_data.py \\\n    --dataset chinese_corpus \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --gpu 0\n</code></pre></p> <p>Qwen models handle Chinese tokenization internally.</p> <p>Training: <pre><code>python run_pipeline.py \\\n    --dataset chinese_corpus \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language zh\n</code></pre></p> <p>The <code>--language zh</code> setting ensures Chinese fonts in visualizations.</p>"},{"location":"advanced/chinese-data/#chinese-visualization","title":"Chinese Visualization","text":"<p>Chinese visualizations require proper font configuration:</p> <pre><code>python -m visualization.run_visualization \\\n    --result_dir /root/autodl-tmp/result/0.6B \\\n    --dataset chinese_corpus \\\n    --mode zero_shot \\\n    --model_size 0.6B \\\n    --language zh \\\n    --dpi 300\n</code></pre> <p>The visualization module automatically: - Selects Chinese-compatible fonts - Handles character encoding - Adjusts layout for Chinese text - Renders word clouds with Chinese characters</p>"},{"location":"advanced/chinese-data/#chinese-english-mixed-data","title":"Chinese-English Mixed Data","text":"<p>For datasets containing both languages:</p> <ol> <li>Clean as Chinese (preserves both languages)</li> <li>Preprocess normally (Qwen handles multilingual)</li> <li>Train with appropriate language setting</li> <li>Visualizations may show mixed text</li> </ol> <p>Primary language should be specified in <code>--language</code> parameter based on majority content.</p>"},{"location":"advanced/custom-datasets/","title":"Working with Custom Datasets","text":"<p>This guide covers complete workflows for processing new datasets.</p>"},{"location":"advanced/custom-datasets/#complete-workflow-for-english-data","title":"Complete Workflow for English Data","text":"<p>Example using a new dataset named <code>news_articles</code>:</p> <p>Step 1: Create dataset directory</p> <pre><code>mkdir -p /root/autodl-tmp/data/news_articles\n</code></pre> <p>Step 2: Place cleaned CSV file</p> <pre><code>cp /path/to/cleaned_news.csv /root/autodl-tmp/data/news_articles/news_articles_cleaned.csv\n</code></pre> <p>File naming convention: <code>{dataset_name}_cleaned.csv</code></p> <p>Step 3: Preprocess data</p> <pre><code>cd /root/autodl-tmp/ETM\n\npython prepare_data.py \\\n    --dataset news_articles \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --max_length 512 \\\n    --gpu 0\n</code></pre> <p>Step 4: Verify preprocessed files</p> <pre><code>python prepare_data.py \\\n    --dataset news_articles \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --check-only\n</code></pre> <p>Step 5: Train model</p> <pre><code>python run_pipeline.py \\\n    --dataset news_articles \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002 \\\n    --kl_start 0.0 \\\n    --kl_end 1.0 \\\n    --kl_warmup 50 \\\n    --patience 10 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>Step 6: Review results</p> <p>Results location: <pre><code>/root/autodl-tmp/result/0.6B/news_articles/zero_shot/\n\u251c\u2500\u2500 metrics/evaluation_results.json\n\u2514\u2500\u2500 visualizations/\n</code></pre></p>"},{"location":"advanced/custom-datasets/#complete-workflow-for-chinese-data","title":"Complete Workflow for Chinese Data","text":"<p>Example using a dataset named <code>weibo_posts</code>:</p> <p>Step 1-2: Setup</p> <pre><code>mkdir -p /root/autodl-tmp/data/weibo_posts\ncp /path/to/cleaned_weibo.csv /root/autodl-tmp/data/weibo_posts/weibo_posts_cleaned.csv\n</code></pre> <p>Step 3: Preprocess Chinese data</p> <pre><code>cd /root/autodl-tmp/ETM\n\npython prepare_data.py \\\n    --dataset weibo_posts \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --max_length 512 \\\n    --gpu 0\n</code></pre> <p>Qwen models handle Chinese natively without special configuration.</p> <p>Step 4: Train with Chinese language setting</p> <pre><code>python run_pipeline.py \\\n    --dataset weibo_posts \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language zh\n</code></pre> <p>The <code>--language zh</code> parameter ensures proper font rendering in visualizations.</p>"},{"location":"advanced/custom-datasets/#starting-from-raw-data","title":"Starting from Raw Data","text":"<p>Process uncleaned data in a single pipeline:</p> <p>English raw data:</p> <pre><code>cd /root/autodl-tmp/ETM\n\npython prepare_data.py \\\n    --dataset news_articles \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --max_length 512 \\\n    --clean \\\n    --raw-input /root/autodl-tmp/data/news_articles/raw_data.csv \\\n    --language english \\\n    --gpu 0\n</code></pre> <p>Chinese raw data:</p> <pre><code>python prepare_data.py \\\n    --dataset weibo_posts \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --max_length 512 \\\n    --clean \\\n    --raw-input /root/autodl-tmp/data/weibo_posts/raw_data.csv \\\n    --language chinese \\\n    --gpu 0\n</code></pre>"},{"location":"advanced/custom-datasets/#supervised-learning-scenario","title":"Supervised Learning Scenario","text":"<p>For datasets with labels in a <code>label</code> or <code>category</code> column:</p> <p>Step 1: Verify data format</p> <p>CSV must contain: <pre><code>text,label\n\"Article about climate policy\",Environment\n\"Report on AI advances\",Technology\n</code></pre></p> <p>Step 2: Preprocess in supervised mode</p> <pre><code>python prepare_data.py \\\n    --dataset labeled_news \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode supervised \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --gpu 0\n</code></pre> <p>Step 3: Train with supervision</p> <pre><code>python run_pipeline.py \\\n    --dataset labeled_news \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode supervised \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language en\n</code></pre>"},{"location":"advanced/custom-datasets/#temporal-data-processing","title":"Temporal Data Processing","text":"<p>For DTM analysis, data must include temporal information:</p> <p>Step 1: Verify temporal column</p> <p>CSV format: <pre><code>text,year\n\"Article from 2020\",2020\n\"Article from 2021\",2021\n</code></pre></p> <p>Accepted column names: <code>year</code>, <code>timestamp</code>, <code>date</code></p> <p>Step 2: Preprocess with time column</p> <pre><code>python prepare_data.py \\\n    --dataset temporal_news \\\n    --model dtm \\\n    --vocab_size 5000 \\\n    --time_column year\n</code></pre> <p>Step 3: Train DTM model</p> <pre><code>python run_pipeline.py \\\n    --dataset temporal_news \\\n    --models dtm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language en\n</code></pre>"},{"location":"advanced/custom-datasets/#pipeline-control","title":"Pipeline Control","text":""},{"location":"advanced/custom-datasets/#skipping-stages","title":"Skipping Stages","text":"<p>Skip training (evaluate existing model): <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --skip-train \\\n    --gpu 0\n</code></pre></p> <p>Skip visualization: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --skip-viz \\\n    --gpu 0\n</code></pre></p> <p>Training only (no evaluation or visualization): <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --skip-eval \\\n    --skip-viz \\\n    --gpu 0\n</code></pre></p>"},{"location":"advanced/custom-datasets/#bow-only-generation","title":"BOW-Only Generation","text":"<p>Generate only bag-of-words without embeddings:</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --bow-only\n</code></pre>"},{"location":"advanced/distributed-training/","title":"Distributed &amp; Memory-Efficient Training","text":"<p>Guide for scaling THETA to larger datasets and constrained environments.</p>"},{"location":"advanced/distributed-training/#memory-efficient-training","title":"Memory-Efficient Training","text":"<p>For limited VRAM:</p> <p>0.6B with reduced batch size: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --batch_size 32 \\\n    --gpu 0\n</code></pre></p> <p>4B with minimal batch size: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 4B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --batch_size 16 \\\n    --gpu 0\n</code></pre></p> <p>8B requiring high-end GPU: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 8B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --batch_size 8 \\\n    --gpu 0\n</code></pre></p> <p>Reduce batch size if out-of-memory errors occur.</p>"},{"location":"advanced/distributed-training/#memory-requirements","title":"Memory Requirements","text":"Model Size Batch Size VRAM Required 0.6B 16 ~6GB 0.6B 32 ~8GB 0.6B 64 ~12GB 4B 8 ~10GB 4B 16 ~14GB 4B 32 ~22GB 8B 8 ~18GB 8B 16 ~28GB"},{"location":"advanced/distributed-training/#multi-gpu-processing","title":"Multi-GPU Processing","text":"<p>Train different configurations in parallel using separate GPUs:</p> <pre><code># Terminal 1\nCUDA_VISIBLE_DEVICES=0 python run_pipeline.py \\\n    --dataset dataset1 --models theta --gpu 0 &amp;\n\n# Terminal 2  \nCUDA_VISIBLE_DEVICES=1 python run_pipeline.py \\\n    --dataset dataset2 --models theta --gpu 0 &amp;\n\n# Terminal 3\nCUDA_VISIBLE_DEVICES=2 python run_pipeline.py \\\n    --dataset dataset3 --models theta --gpu 0 &amp;\n</code></pre> <p>Each process uses a different GPU.</p>"},{"location":"advanced/distributed-training/#environment-variables","title":"Environment Variables","text":""},{"location":"advanced/distributed-training/#cuda-configuration","title":"CUDA Configuration","text":"<pre><code># Use specific GPU\nCUDA_VISIBLE_DEVICES=0 python run_pipeline.py --dataset my_dataset --models theta\n\n# Limit GPU memory fraction\nPYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512 python run_pipeline.py ...\n\n# Enable memory debugging\nPYTORCH_NO_CUDA_MEMORY_CACHING=1 python run_pipeline.py ...\n</code></pre>"},{"location":"advanced/distributed-training/#logging","title":"Logging","text":"<pre><code># Disable progress bars\nTQDM_DISABLE=1 python run_pipeline.py ...\n\n# Reduce logging\nexport PYTHONWARNINGS=\"ignore\"\npython run_pipeline.py ...\n</code></pre>"},{"location":"advanced/hyperparameters/","title":"Hyperparameter Tuning","text":"<p>Systematic guide to optimizing THETA hyperparameters.</p>"},{"location":"advanced/hyperparameters/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>Conservative approach (unstable training): <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --learning_rate 0.0005 \\\n    --epochs 150 \\\n    --gpu 0\n</code></pre></p> <p>Standard approach: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --learning_rate 0.002 \\\n    --epochs 100 \\\n    --gpu 0\n</code></pre></p> <p>Aggressive approach (slow convergence): <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --learning_rate 0.01 \\\n    --epochs 80 \\\n    --gpu 0\n</code></pre></p> <p>Monitor training loss curves to determine if adjustment is needed.</p>"},{"location":"advanced/hyperparameters/#batch-size-optimization","title":"Batch Size Optimization","text":"Batch Size Advantages Disadvantages 32 Lower memory, better exploration Noisy updates, slower convergence 64 Balanced (default) \u2014 128 Stable updates, faster epochs Higher memory, may overfit"},{"location":"advanced/hyperparameters/#kl-annealing-strategies","title":"KL Annealing Strategies","text":"<p>No annealing (immediate full KL): <code>--kl_start 1.0 --kl_end 1.0 --kl_warmup 0</code> Risk: Posterior collapse, poor topic quality</p> <p>Standard annealing (recommended): <code>--kl_start 0.0 --kl_end 1.0 --kl_warmup 50</code></p> <p>Slow annealing (complex data): <code>--kl_start 0.0 --kl_end 1.0 --kl_warmup 80</code></p> <p>Partial annealing (fine-tuning): <code>--kl_start 0.2 --kl_end 0.8 --kl_warmup 40</code></p>"},{"location":"advanced/hyperparameters/#hidden-dimension-tuning","title":"Hidden Dimension Tuning","text":"Hidden Dim Use Case 256 Small datasets or memory constrained 512 Default choice for most applications 1024 Large complex datasets when VRAM permits"},{"location":"advanced/hyperparameters/#early-stopping-configuration","title":"Early Stopping Configuration","text":"Patience Behavior 5 Stops quickly if validation loss plateaus 10 Default setting 20 Allows longer training before stopping Disabled (<code>--no_early_stopping</code>) Trains for all specified epochs"},{"location":"advanced/hyperparameters/#vocabulary-size-selection","title":"Vocabulary Size Selection","text":"Corpus Size Vocabulary Size Coverage &lt; 1K docs 2000-3000 ~85% 1K-10K docs 5000 ~90% 10K-100K docs 8000-10000 ~92% &gt; 100K docs 10000-15000 ~95%"},{"location":"advanced/hyperparameters/#using-different-model-sizes","title":"Using Different Model Sizes","text":""},{"location":"advanced/hyperparameters/#scaling-strategy","title":"Scaling Strategy","text":"<p>Development workflow: 1. Start with 0.6B model 2. Optimize hyperparameters 3. Scale to 4B for production 4. Use 8B for final results if needed</p> <p>Quick comparison: <pre><code>for size in 0.6B 4B 8B; do\n    python run_pipeline.py \\\n        --dataset my_dataset \\\n        --models theta \\\n        --model_size $size \\\n        --mode zero_shot \\\n        --num_topics 20 \\\n        --gpu 0\ndone\n</code></pre></p>"},{"location":"advanced/hyperparameters/#quality-vs-cost-analysis","title":"Quality vs Cost Analysis","text":"<p>0.6B \u2192 4B: - Topic diversity: +3-5% - Coherence (NPMI): +10-15% - Training time: +60-80%</p> <p>4B \u2192 8B: - Topic diversity: +1-2% - Coherence (NPMI): +5-8% - Training time: +80-100%</p> <p>Diminishing returns suggest 4B is often the best choice for production.</p>"},{"location":"advanced/hyperparameters/#grid-search","title":"Grid Search","text":"<p>Systematic hyperparameter exploration:</p> <pre><code>#!/bin/bash\ntopics=(15 20 25 30)\nlearning_rates=(0.001 0.002 0.005)\nhidden_dims=(256 512 768)\n\nfor K in \"${topics[@]}\"; do\n    for lr in \"${learning_rates[@]}\"; do\n        for hd in \"${hidden_dims[@]}\"; do\n            echo \"Training K=$K, lr=$lr, hd=$hd\"\n\n            python run_pipeline.py \\\n                --dataset my_dataset \\\n                --models theta \\\n                --model_size 0.6B \\\n                --mode zero_shot \\\n                --num_topics $K \\\n                --learning_rate $lr \\\n                --hidden_dim $hd \\\n                --epochs 100 \\\n                --batch_size 64 \\\n                --gpu 0\n\n            mkdir -p results_grid/K${K}_lr${lr}_hd${hd}\n            cp -r result/0.6B/my_dataset/zero_shot/* results_grid/K${K}_lr${lr}_hd${hd}/\n        done\n    done\ndone\n</code></pre>"},{"location":"advanced/hyperparameters/#batch-processing-multiple-datasets","title":"Batch Processing Multiple Datasets","text":"<pre><code>#!/bin/bash\ndatasets=(\"news\" \"reviews\" \"papers\" \"social\")\n\nfor dataset in \"${datasets[@]}\"; do\n    echo \"Processing $dataset...\"\n\n    python prepare_data.py \\\n        --dataset $dataset \\\n        --model theta \\\n        --model_size 0.6B \\\n        --mode zero_shot \\\n        --vocab_size 5000 \\\n        --gpu 0\n\n    python run_pipeline.py \\\n        --dataset $dataset \\\n        --models theta \\\n        --model_size 0.6B \\\n        --mode zero_shot \\\n        --num_topics 20 \\\n        --gpu 0\ndone\n</code></pre>"},{"location":"advanced/hyperparameters/#parallel-processing-on-multiple-gpus","title":"Parallel Processing on Multiple GPUs","text":"<pre><code># Terminal 1\nCUDA_VISIBLE_DEVICES=0 python run_pipeline.py \\\n    --dataset dataset1 --models theta --gpu 0 &amp;\n\n# Terminal 2  \nCUDA_VISIBLE_DEVICES=1 python run_pipeline.py \\\n    --dataset dataset2 --models theta --gpu 0 &amp;\n\n# Terminal 3\nCUDA_VISIBLE_DEVICES=2 python run_pipeline.py \\\n    --dataset dataset3 --models theta --gpu 0 &amp;\n</code></pre> <p>Each process uses a different GPU.</p>"},{"location":"api/prepare-data/","title":"prepare_data.py","text":"<p>Data preprocessing script for generating embeddings and bag-of-words representations.</p>"},{"location":"api/prepare-data/#basic-usage","title":"Basic Usage","text":"<pre><code>python prepare_data.py --dataset DATASET --model MODEL [OPTIONS]\n</code></pre>"},{"location":"api/prepare-data/#required-parameters","title":"Required Parameters","text":"Parameter Type Description <code>--dataset</code> string Dataset name (must match directory name in <code>/root/autodl-tmp/data/</code>) <code>--model</code> string Model type: <code>theta</code>, <code>baseline</code>, or <code>dtm</code>"},{"location":"api/prepare-data/#model-configuration","title":"Model Configuration","text":"Parameter Type Default Description <code>--model_size</code> string <code>0.6B</code> Qwen model size: <code>0.6B</code>, <code>4B</code>, or <code>8B</code> (THETA only) <code>--mode</code> string <code>zero_shot</code> Training mode: <code>zero_shot</code>, <code>supervised</code>, or <code>unsupervised</code> (THETA only)"},{"location":"api/prepare-data/#data-processing","title":"Data Processing","text":"Parameter Type Default Range Description <code>--vocab_size</code> int <code>5000</code> 1000-20000 Vocabulary size for BOW representation <code>--batch_size</code> int <code>32</code> 8-128 Batch size for embedding generation <code>--max_length</code> int <code>512</code> 128-2048 Maximum sequence length for embeddings"},{"location":"api/prepare-data/#gpu-configuration","title":"GPU Configuration","text":"Parameter Type Default Description <code>--gpu</code> int <code>0</code> GPU device ID (0, 1, 2, ...)"},{"location":"api/prepare-data/#data-cleaning","title":"Data Cleaning","text":"Parameter Type Default Description <code>--clean</code> flag False Clean data before preprocessing <code>--raw-input</code> string None Path to raw CSV file (requires <code>--clean</code>) <code>--language</code> string <code>english</code> Cleaning language: <code>english</code> or <code>chinese</code>"},{"location":"api/prepare-data/#utility-options","title":"Utility Options","text":"Parameter Type Default Description <code>--bow-only</code> flag False Generate BOW only, skip embeddings <code>--check-only</code> flag False Check if preprocessed files exist <code>--time_column</code> string <code>year</code> Time column name for DTM (DTM only)"},{"location":"api/prepare-data/#examples","title":"Examples","text":"<p>Basic THETA preprocessing: <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000\n</code></pre></p> <p>Baseline model preprocessing: <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model baseline \\\n    --vocab_size 5000\n</code></pre></p> <p>Combined cleaning and preprocessing: <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --clean \\\n    --raw-input /path/to/raw.csv \\\n    --language english\n</code></pre></p> <p>Check preprocessed files: <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --check-only\n</code></pre></p>"},{"location":"api/prepare-data/#output-files","title":"Output Files","text":"<p>Preprocessed data is saved to: <pre><code>/root/autodl-tmp/result/{model_size}/{dataset}/bow/\n</code></pre></p> <p>Generated files: - <code>qwen_embeddings_{mode}.npy</code>: Document embeddings - <code>vocab.pkl</code>: Vocabulary dictionary - <code>doc_indices.npy</code>: Document-term indices - <code>bow_matrix.npz</code>: Sparse BOW matrix</p>"},{"location":"api/run-pipeline/","title":"run_pipeline.py","text":"<p>Unified training, evaluation, and visualization pipeline.</p>"},{"location":"api/run-pipeline/#basic-usage","title":"Basic Usage","text":"<pre><code>python run_pipeline.py --dataset DATASET --models MODELS [OPTIONS]\n</code></pre>"},{"location":"api/run-pipeline/#required-parameters","title":"Required Parameters","text":"Parameter Type Description <code>--dataset</code> string Dataset name <code>--models</code> string Comma-separated model list: <code>theta</code>, <code>lda</code>, <code>etm</code>, <code>ctm</code>, <code>dtm</code>"},{"location":"api/run-pipeline/#model-configuration-theta","title":"Model Configuration (THETA)","text":"Parameter Type Default Description <code>--model_size</code> string <code>0.6B</code> Qwen model size: <code>0.6B</code>, <code>4B</code>, or <code>8B</code> <code>--mode</code> string <code>zero_shot</code> Training mode: <code>zero_shot</code>, <code>supervised</code>, or <code>unsupervised</code>"},{"location":"api/run-pipeline/#topic-model-parameters","title":"Topic Model Parameters","text":"Parameter Type Default Range Description <code>--num_topics</code> int <code>20</code> 5-100 Number of topics to discover <code>--epochs</code> int <code>100</code> 10-500 Maximum training epochs <code>--batch_size</code> int <code>64</code> 8-512 Training batch size"},{"location":"api/run-pipeline/#neural-network-architecture","title":"Neural Network Architecture","text":"Parameter Type Default Range Description <code>--hidden_dim</code> int <code>512</code> 128-1024 Encoder hidden dimension"},{"location":"api/run-pipeline/#optimization","title":"Optimization","text":"Parameter Type Default Range Description <code>--learning_rate</code> float <code>0.002</code> 0.00001-0.1 Learning rate for optimizer"},{"location":"api/run-pipeline/#kl-annealing","title":"KL Annealing","text":"Parameter Type Default Range Description <code>--kl_start</code> float <code>0.0</code> 0.0-1.0 Initial KL divergence weight <code>--kl_end</code> float <code>1.0</code> 0.0-1.0 Final KL divergence weight <code>--kl_warmup</code> int <code>50</code> 0-200 Number of warmup epochs for KL annealing"},{"location":"api/run-pipeline/#early-stopping","title":"Early Stopping","text":"Parameter Type Default Range Description <code>--patience</code> int <code>10</code> 1-50 Epochs to wait before early stopping <code>--no_early_stopping</code> flag False N/A Disable early stopping"},{"location":"api/run-pipeline/#hardware-configuration","title":"Hardware Configuration","text":"Parameter Type Default Description <code>--gpu</code> int <code>0</code> GPU device ID"},{"location":"api/run-pipeline/#output-configuration","title":"Output Configuration","text":"Parameter Type Default Description <code>--language</code> string <code>en</code> Visualization language: <code>en</code> or <code>zh</code>"},{"location":"api/run-pipeline/#pipeline-control","title":"Pipeline Control","text":"Parameter Type Default Description <code>--skip-train</code> flag False Skip training, evaluate only <code>--skip-eval</code> flag False Skip evaluation <code>--skip-viz</code> flag False Skip visualization <code>--check-only</code> flag False Check data files only <code>--prepare</code> flag False Run preprocessing before training"},{"location":"api/run-pipeline/#examples","title":"Examples","text":"<p>Basic THETA training: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --gpu 0\n</code></pre></p> <p>Multiple baseline models: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models lda,etm,ctm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --gpu 0\n</code></pre></p> <p>Custom hyperparameters: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 30 \\\n    --epochs 150 \\\n    --batch_size 32 \\\n    --hidden_dim 768 \\\n    --learning_rate 0.001 \\\n    --kl_start 0.0 \\\n    --kl_end 1.0 \\\n    --kl_warmup 80 \\\n    --patience 15 \\\n    --gpu 0\n</code></pre></p> <p>Evaluate existing model: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --skip-train \\\n    --gpu 0\n</code></pre></p>"},{"location":"api/run-pipeline/#output-files","title":"Output Files","text":"<p>THETA models: <pre><code>/root/autodl-tmp/result/{model_size}/{dataset}/{mode}/\n\u251c\u2500\u2500 checkpoints/\n\u2502   \u251c\u2500\u2500 best_model.pt\n\u2502   \u2514\u2500\u2500 training_history.json\n\u251c\u2500\u2500 metrics/\n\u2502   \u2514\u2500\u2500 evaluation_results.json\n\u2514\u2500\u2500 visualizations/\n    \u251c\u2500\u2500 topic_words_bars.png\n    \u251c\u2500\u2500 topic_similarity.png\n    \u251c\u2500\u2500 doc_topic_umap.png\n    \u251c\u2500\u2500 topic_wordclouds.png\n    \u251c\u2500\u2500 metrics.png\n    \u2514\u2500\u2500 pyldavis.html\n</code></pre></p> <p>Baseline models: <pre><code>/root/autodl-tmp/result/baseline/{dataset}/{model}/K{num_topics}/\n\u251c\u2500\u2500 checkpoints/\n\u251c\u2500\u2500 metrics/\n\u2514\u2500\u2500 visualizations/\n</code></pre></p>"},{"location":"api/run-pipeline/#return-codes","title":"Return Codes","text":"Exit Code Meaning 0 Success 1 General error 2 File not found 3 Invalid parameters 4 CUDA out of memory 5 Convergence failure"},{"location":"api/visualization/","title":"visualization.run_visualization","text":"<p>Separate visualization generation tool.</p>"},{"location":"api/visualization/#basic-usage","title":"Basic Usage","text":"<pre><code>python -m visualization.run_visualization --result_dir DIR --dataset DATASET [OPTIONS]\n</code></pre>"},{"location":"api/visualization/#required-parameters","title":"Required Parameters","text":"Parameter Type Description <code>--result_dir</code> string Results directory path <code>--dataset</code> string Dataset name"},{"location":"api/visualization/#theta-model-parameters","title":"THETA Model Parameters","text":"Parameter Type Default Description <code>--mode</code> string <code>zero_shot</code> Training mode (for THETA models) <code>--model_size</code> string <code>0.6B</code> Qwen model size (for THETA models)"},{"location":"api/visualization/#baseline-model-parameters","title":"Baseline Model Parameters","text":"Parameter Type Default Description <code>--baseline</code> flag False Indicates baseline model <code>--model</code> string None Baseline model name: <code>lda</code>, <code>etm</code>, <code>ctm</code>, or <code>dtm</code> <code>--num_topics</code> int <code>20</code> Number of topics (for baseline models)"},{"location":"api/visualization/#output-configuration","title":"Output Configuration","text":"Parameter Type Default Description <code>--language</code> string <code>en</code> Visualization language: <code>en</code> or <code>zh</code> <code>--dpi</code> int <code>300</code> Image resolution (dots per inch)"},{"location":"api/visualization/#examples","title":"Examples","text":"<p>THETA model visualization: <pre><code>python -m visualization.run_visualization \\\n    --result_dir /root/autodl-tmp/result/0.6B \\\n    --dataset my_dataset \\\n    --mode zero_shot \\\n    --model_size 0.6B \\\n    --language en \\\n    --dpi 300\n</code></pre></p> <p>LDA model visualization: <pre><code>python -m visualization.run_visualization \\\n    --baseline \\\n    --result_dir /root/autodl-tmp/result/baseline \\\n    --dataset my_dataset \\\n    --model lda \\\n    --num_topics 20 \\\n    --language en \\\n    --dpi 300\n</code></pre></p> <p>High-resolution visualization: <pre><code>python -m visualization.run_visualization \\\n    --result_dir /root/autodl-tmp/result/0.6B \\\n    --dataset my_dataset \\\n    --mode zero_shot \\\n    --model_size 0.6B \\\n    --language en \\\n    --dpi 600\n</code></pre></p> <p>Chinese visualization: <pre><code>python -m visualization.run_visualization \\\n    --result_dir /root/autodl-tmp/result/0.6B \\\n    --dataset chinese_dataset \\\n    --mode zero_shot \\\n    --model_size 0.6B \\\n    --language zh \\\n    --dpi 300\n</code></pre></p>"},{"location":"api/visualization/#output-files","title":"Output Files","text":"<p>Visualizations are saved to the same directory as the model results: - <code>topic_words_bars.png</code>: Bar charts of topic words - <code>topic_similarity.png</code>: Topic similarity heatmap - <code>doc_topic_umap.png</code>: Document-topic UMAP projection - <code>topic_wordclouds.png</code>: Word clouds for each topic - <code>metrics.png</code>: Evaluation metrics comparison - <code>pyldavis.html</code>: Interactive visualization</p>"},{"location":"api/visualization/#datacleanmain","title":"dataclean.main","text":"<p>Data cleaning module for preprocessing raw text.</p>"},{"location":"api/visualization/#basic-usage_1","title":"Basic Usage","text":"<pre><code>python -m dataclean.main --input INPUT --output OUTPUT --language LANGUAGE\n</code></pre>"},{"location":"api/visualization/#parameters","title":"Parameters","text":"Parameter Type Description <code>--input</code> string Input CSV file path or directory <code>--output</code> string Output CSV file path or directory <code>--language</code> string Language: <code>english</code> or <code>chinese</code>"},{"location":"api/visualization/#examples_1","title":"Examples","text":"<p>Clean single file (English): <pre><code>python -m dataclean.main \\\n    --input /root/autodl-tmp/data/raw_data.csv \\\n    --output /root/autodl-tmp/data/cleaned_data.csv \\\n    --language english\n</code></pre></p> <p>Clean single file (Chinese): <pre><code>python -m dataclean.main \\\n    --input /root/autodl-tmp/data/raw_data.csv \\\n    --output /root/autodl-tmp/data/cleaned_data.csv \\\n    --language chinese\n</code></pre></p> <p>Clean directory: <pre><code>python -m dataclean.main \\\n    --input /root/autodl-tmp/data/raw/ \\\n    --output /root/autodl-tmp/data/cleaned/ \\\n    --language english\n</code></pre></p>"},{"location":"api/visualization/#cleaning-operations","title":"Cleaning Operations","text":"<p>English cleaning: - Remove HTML tags and entities - Remove URLs and email addresses - Remove special characters (except basic punctuation) - Normalize whitespace - Remove non-ASCII characters (optional) - Lowercase text (optional)</p> <p>Chinese cleaning: - Remove HTML tags and entities - Remove URLs and email addresses - Normalize full-width and half-width characters - Handle Chinese punctuation - Remove non-Chinese characters (optional) - Preserve word boundaries</p>"},{"location":"appendix/faq/","title":"Appendix","text":"<p>Reference materials and supplementary information.</p>"},{"location":"appendix/faq/#complete-parameter-reference","title":"Complete Parameter Reference","text":""},{"location":"appendix/faq/#prepare_datapy","title":"prepare_data.py","text":"Parameter Type Default Range/Options Required Description <code>--dataset</code> string - - Yes Dataset name <code>--model</code> string - theta/baseline/dtm Yes Model type <code>--model_size</code> string 0.6B 0.6B/4B/8B No Qwen model size <code>--mode</code> string zero_shot zero_shot/supervised/unsupervised No Training mode <code>--vocab_size</code> int 5000 1000-20000 No Vocabulary size <code>--batch_size</code> int 32 8-128 No Batch size <code>--max_length</code> int 512 128-2048 No Max sequence length <code>--gpu</code> int 0 0-7 No GPU device ID <code>--clean</code> flag False - No Clean data first <code>--raw-input</code> string None filepath No Raw CSV path <code>--language</code> string english english/chinese No Cleaning language <code>--bow-only</code> flag False - No BOW only <code>--check-only</code> flag False - No Check files only <code>--time_column</code> string year column name No Time column (DTM)"},{"location":"appendix/faq/#run_pipelinepy","title":"run_pipeline.py","text":"Parameter Type Default Range/Options Required Description <code>--dataset</code> string - - Yes Dataset name <code>--models</code> string - theta,lda,etm,ctm,dtm Yes Model list <code>--model_size</code> string 0.6B 0.6B/4B/8B No Qwen model size <code>--mode</code> string zero_shot zero_shot/supervised/unsupervised No Training mode <code>--num_topics</code> int 20 5-100 No Number of topics <code>--epochs</code> int 100 10-500 No Training epochs <code>--batch_size</code> int 64 8-512 No Batch size <code>--hidden_dim</code> int 512 128-1024 No Hidden dimension <code>--learning_rate</code> float 0.002 0.00001-0.1 No Learning rate <code>--kl_start</code> float 0.0 0.0-1.0 No KL start weight <code>--kl_end</code> float 1.0 0.0-1.0 No KL end weight <code>--kl_warmup</code> int 50 0-200 No KL warmup epochs <code>--patience</code> int 10 1-50 No Early stopping patience <code>--no_early_stopping</code> flag False - No Disable early stopping <code>--gpu</code> int 0 0-7 No GPU device ID <code>--language</code> string en en/zh No Visualization language <code>--skip-train</code> flag False - No Skip training <code>--skip-eval</code> flag False - No Skip evaluation <code>--skip-viz</code> flag False - No Skip visualization"},{"location":"appendix/faq/#visualizationrun_visualization","title":"visualization.run_visualization","text":"Parameter Type Default Range/Options Required Description <code>--result_dir</code> string - directory Yes Results directory <code>--dataset</code> string - - Yes Dataset name <code>--mode</code> string zero_shot zero_shot/supervised/unsupervised No THETA mode <code>--model_size</code> string 0.6B 0.6B/4B/8B No Model size <code>--baseline</code> flag False - No Baseline flag <code>--model</code> string None lda/etm/ctm/dtm No Baseline model <code>--num_topics</code> int 20 5-100 No Number of topics <code>--language</code> string en en/zh No Language <code>--dpi</code> int 300 72-1200 No Image resolution"},{"location":"appendix/faq/#directory-structure","title":"Directory Structure","text":"<pre><code>/root/autodl-tmp/\n\u251c\u2500\u2500 ETM/\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 run_pipeline.py\n\u2502   \u251c\u2500\u2500 prepare_data.py\n\u2502   \u2514\u2500\u2500 src/\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 {dataset}/\n\u2502       \u2514\u2500\u2500 {dataset}_cleaned.csv\n\u251c\u2500\u2500 result/\n\u2502   \u251c\u2500\u2500 0.6B/\n\u2502   \u251c\u2500\u2500 4B/\n\u2502   \u251c\u2500\u2500 8B/\n\u2502   \u2514\u2500\u2500 baseline/\n\u2514\u2500\u2500 embedding_models/\n</code></pre>"},{"location":"appendix/faq/#hardware-requirements","title":"Hardware Requirements","text":"Setup CPU RAM GPU CUDA Storage Minimum 4 cores 8GB 4GB VRAM 11.8+ 20GB Recommended 8 cores 16GB 12GB VRAM 12.1+ 50GB SSD High-Performance 16+ cores 32GB+ A100 40GB 12.1+ 200GB NVMe"},{"location":"appendix/faq/#faq","title":"FAQ","text":"<p>Q: What makes THETA different? A: THETA uses Qwen embeddings and neural variational inference for better semantic understanding than LDA or ETM.</p> <p>Q: Which model size to use? A: 0.6B for prototyping, 4B for production, 8B for maximum quality.</p> <p>Q: Minimum dataset size? A: 500+ documents with 50+ words average recommended.</p> <p>Q: Training time? A: 5K docs with 0.6B on V100: ~25 min. 4B: ~50 min.</p> <p>Q: GPU required? A: Yes. GPU required for preprocessing and training.</p>"},{"location":"appendix/faq/#citation","title":"Citation","text":"<pre><code>@article{theta2024,\n  title={THETA: Advanced Topic Modeling with Qwen Embeddings},\n  author={CodeSoul Team},\n  year={2024},\n  url={https://github.com/CodeSoul-co/THETA}\n}\n</code></pre>"},{"location":"appendix/faq/#contact","title":"Contact","text":"<ul> <li>Website: https://theta.code-soul.com</li> <li>GitHub: https://github.com/CodeSoul-co/THETA</li> <li>Email: support@theta.code-soul.com</li> </ul> <p>Document Version: 1.0.0 Last Updated: February 6, 2026</p>"},{"location":"examples/chinese-dataset/","title":"Chinese Dataset Example","text":"<p>This example demonstrates Chinese text processing with THETA.</p>"},{"location":"examples/chinese-dataset/#dataset-description","title":"Dataset Description","text":"<ul> <li>Domain: Weibo posts</li> <li>Size: 8000 documents</li> <li>Language: Chinese</li> <li>Source: Weibo public API</li> <li>Topics: Various social discussions</li> </ul>"},{"location":"examples/chinese-dataset/#step-1-data-cleaning","title":"Step 1: Data Cleaning","text":"<p>Clean raw Chinese text:</p> <pre><code>cd /root/autodl-tmp/ETM\n\npython -m dataclean.main \\\n    --input /root/autodl-tmp/data/weibo/raw_data.csv \\\n    --output /root/autodl-tmp/data/weibo/weibo_cleaned.csv \\\n    --language chinese\n</code></pre> <p>Cleaning removes: - URLs and mentions - Special symbols - Excessive punctuation - Non-Chinese characters</p>"},{"location":"examples/chinese-dataset/#step-2-preprocess","title":"Step 2: Preprocess","text":"<pre><code>python prepare_data.py \\\n    --dataset weibo \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --max_length 512 \\\n    --gpu 0\n</code></pre> <p>Qwen models handle Chinese tokenization natively.</p>"},{"location":"examples/chinese-dataset/#step-3-train-model","title":"Step 3: Train Model","text":"<pre><code>python run_pipeline.py \\\n    --dataset weibo \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002 \\\n    --gpu 0 \\\n    --language zh\n</code></pre> <p>Note: <code>--language zh</code> ensures proper Chinese font rendering.</p>"},{"location":"examples/chinese-dataset/#step-4-results","title":"Step 4: Results","text":"<p>Discovered topics include: - \u751f\u6d3b, \u5206\u4eab, \u65e5\u5e38, \u4eca\u5929, \u5f00\u5fc3 (daily life) - \u5de5\u4f5c, \u516c\u53f8, \u540c\u4e8b, \u52a0\u73ed, \u9879\u76ee (work) - \u7f8e\u98df, \u9910\u5385, \u597d\u5403, \u63a8\u8350, \u5473\u9053 (food) - \u65c5\u6e38, \u666f\u70b9, \u98ce\u666f, \u62cd\u7167, \u7f8e\u4e3d (travel)</p> <p>Visualizations render Chinese characters correctly with appropriate fonts.</p>"},{"location":"examples/chinese-dataset/#step-5-temporal-analysis","title":"Step 5: Temporal Analysis","text":"<p>If Weibo data includes timestamps, use DTM:</p> <pre><code>python prepare_data.py \\\n    --dataset weibo \\\n    --model dtm \\\n    --vocab_size 5000 \\\n    --time_column year\n\npython run_pipeline.py \\\n    --dataset weibo \\\n    --models dtm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language zh\n</code></pre> <p>DTM reveals topic evolution over time: - Rise of remote work discussions (2020-2021) - Increasing environmental awareness (2021-2023) - Technology adoption trends (2020-2023)</p>"},{"location":"examples/english-dataset/","title":"English Dataset Examples","text":"<p>Complete tutorials demonstrating THETA usage with English data.</p>"},{"location":"examples/english-dataset/#example-1-english-news-dataset","title":"Example 1: English News Dataset","text":"<p>This example demonstrates the complete workflow for analyzing news articles.</p>"},{"location":"examples/english-dataset/#dataset-description","title":"Dataset Description","text":"<ul> <li>Domain: News articles</li> <li>Size: 5000 documents</li> <li>Language: English</li> <li>Source: Online news aggregator</li> <li>Time period: 2020-2023</li> </ul>"},{"location":"examples/english-dataset/#step-1-prepare-data","title":"Step 1: Prepare Data","text":"<p>Create dataset directory and place CSV file:</p> <pre><code>mkdir -p /root/autodl-tmp/data/news_corpus\n</code></pre> <p>CSV format: <pre><code>text\n\"Federal Reserve raises interest rates to combat inflation...\"\n\"Climate summit reaches historic agreement on emissions...\"\n\"Technology companies announce layoffs amid economic uncertainty...\"\n</code></pre></p> <p>Save as <code>/root/autodl-tmp/data/news_corpus/news_corpus_cleaned.csv</code></p>"},{"location":"examples/english-dataset/#step-2-preprocess","title":"Step 2: Preprocess","text":"<pre><code>cd /root/autodl-tmp/ETM\n\npython prepare_data.py \\\n    --dataset news_corpus \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --max_length 512 \\\n    --gpu 0\n</code></pre> <p>Processing time: ~5 minutes on V100 GPU</p>"},{"location":"examples/english-dataset/#step-3-train-model","title":"Step 3: Train Model","text":"<p>Train with 25 topics to capture diverse news categories:</p> <pre><code>python run_pipeline.py \\\n    --dataset news_corpus \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 25 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002 \\\n    --kl_start 0.0 \\\n    --kl_end 1.0 \\\n    --kl_warmup 50 \\\n    --patience 10 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>Training time: ~25 minutes</p>"},{"location":"examples/english-dataset/#step-4-analyze-results","title":"Step 4: Analyze Results","text":"<pre><code>cat /root/autodl-tmp/result/0.6B/news_corpus/zero_shot/metrics/evaluation_results.json\n</code></pre> <p>Example output: <pre><code>{\n  \"TD\": 0.87,\n  \"iRBO\": 0.73,\n  \"NPMI\": 0.39,\n  \"C_V\": 0.62,\n  \"UMass\": -2.56,\n  \"Exclusivity\": 0.81,\n  \"PPL\": 152.34\n}\n</code></pre></p>"},{"location":"examples/english-dataset/#step-5-compare-with-baselines","title":"Step 5: Compare with Baselines","text":"<pre><code>python prepare_data.py \\\n    --dataset news_corpus \\\n    --model baseline \\\n    --vocab_size 5000\n\npython run_pipeline.py \\\n    --dataset news_corpus \\\n    --models lda,etm,ctm \\\n    --num_topics 25 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>Comparison results:</p> Model TD NPMI C_V PPL LDA 0.72 0.24 0.48 185.2 ETM 0.79 0.31 0.55 168.5 CTM 0.83 0.36 0.59 158.7 THETA 0.87 0.39 0.62 152.3"},{"location":"examples/english-dataset/#example-2-large-scale-dataset-with-4b-model","title":"Example 2: Large-Scale Dataset with 4B Model","text":"<p>This example demonstrates scaling to larger models and datasets.</p>"},{"location":"examples/english-dataset/#dataset-description_1","title":"Dataset Description","text":"<ul> <li>Domain: Wikipedia articles</li> <li>Size: 50000 documents</li> <li>Language: English</li> <li>Complexity: Diverse topics and vocabulary</li> </ul>"},{"location":"examples/english-dataset/#step-1-preprocess-with-4b-model","title":"Step 1: Preprocess with 4B Model","text":"<pre><code>python prepare_data.py \\\n    --dataset wikipedia \\\n    --model theta \\\n    --model_size 4B \\\n    --mode zero_shot \\\n    --vocab_size 10000 \\\n    --batch_size 16 \\\n    --max_length 512 \\\n    --gpu 0\n</code></pre> <p>Processing time: ~4 hours on A100 GPU</p>"},{"location":"examples/english-dataset/#step-2-train-with-increased-capacity","title":"Step 2: Train with Increased Capacity","text":"<pre><code>python run_pipeline.py \\\n    --dataset wikipedia \\\n    --models theta \\\n    --model_size 4B \\\n    --mode zero_shot \\\n    --num_topics 50 \\\n    --epochs 150 \\\n    --batch_size 32 \\\n    --hidden_dim 768 \\\n    --learning_rate 0.001 \\\n    --kl_start 0.0 \\\n    --kl_end 1.0 \\\n    --kl_warmup 80 \\\n    --patience 15 \\\n    --gpu 0 \\\n    --language en\n</code></pre>"},{"location":"examples/english-dataset/#step-3-compare-model-sizes","title":"Step 3: Compare Model Sizes","text":"Model TD NPMI C_V PPL Time 0.6B 0.89 0.43 0.66 138.5 90 min 4B 0.92 0.49 0.71 128.2 180 min <p>4B model provides significant quality improvements at 2x cost.</p>"},{"location":"examples/english-dataset/#example-3-multi-model-comparison","title":"Example 3: Multi-Model Comparison","text":""},{"location":"examples/english-dataset/#dataset-description_2","title":"Dataset Description","text":"<ul> <li>Domain: Movie reviews</li> <li>Size: 4000 documents</li> <li>Language: English</li> </ul>"},{"location":"examples/english-dataset/#step-1-preprocess-for-all-models","title":"Step 1: Preprocess for All Models","text":"<pre><code>python prepare_data.py \\\n    --dataset movie_reviews \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --gpu 0\n\npython prepare_data.py \\\n    --dataset movie_reviews \\\n    --model baseline \\\n    --vocab_size 5000\n</code></pre>"},{"location":"examples/english-dataset/#step-2-train-all-models","title":"Step 2: Train All Models","text":"<pre><code>python run_pipeline.py \\\n    --dataset movie_reviews \\\n    --models lda,etm,ctm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language en\n\npython run_pipeline.py \\\n    --dataset movie_reviews \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language en\n</code></pre>"},{"location":"examples/english-dataset/#step-3-comparison-table","title":"Step 3: Comparison Table","text":"Model TD iRBO NPMI C_V Exclusivity PPL Time LDA 0.74 0.68 0.26 0.49 0.76 178.3 12 min ETM 0.81 0.71 0.33 0.56 0.79 163.5 18 min CTM 0.84 0.74 0.37 0.60 0.82 154.2 22 min THETA 0.88 0.77 0.41 0.64 0.85 147.8 26 min"},{"location":"examples/english-dataset/#example-4-hyperparameter-grid-search","title":"Example 4: Hyperparameter Grid Search","text":""},{"location":"examples/english-dataset/#setup","title":"Setup","text":"<p>Dataset: 2000 news articles Goal: Find optimal hyperparameters for topic quality</p>"},{"location":"examples/english-dataset/#grid-search-script","title":"Grid Search Script","text":"<pre><code>#!/bin/bash\n\ntopics=(15 20 25 30)\nlearning_rates=(0.001 0.002 0.005)\nhidden_dims=(256 512 768)\n\nfor K in \"${topics[@]}\"; do\n    for lr in \"${learning_rates[@]}\"; do\n        for hd in \"${hidden_dims[@]}\"; do\n            echo \"Training K=$K, lr=$lr, hd=$hd\"\n\n            python run_pipeline.py \\\n                --dataset news \\\n                --models theta \\\n                --model_size 0.6B \\\n                --mode zero_shot \\\n                --num_topics $K \\\n                --learning_rate $lr \\\n                --hidden_dim $hd \\\n                --epochs 100 \\\n                --batch_size 64 \\\n                --gpu 0 \\\n                --language en\n\n            mkdir -p results_grid/K${K}_lr${lr}_hd${hd}\n            cp -r result/0.6B/news/zero_shot/* results_grid/K${K}_lr${lr}_hd${hd}/\n        done\n    done\ndone\n</code></pre>"},{"location":"examples/english-dataset/#best-configuration","title":"Best Configuration","text":"<p>Analysis reveals optimal settings: - Number of topics: 20 - Learning rate: 0.002 - Hidden dimension: 512</p>"},{"location":"examples/english-dataset/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"examples/english-dataset/#data-preparation","title":"Data Preparation","text":"<ol> <li>Clean data thoroughly before preprocessing</li> <li>Ensure CSV follows naming convention</li> <li>Verify data quality with exploratory analysis</li> </ol>"},{"location":"examples/english-dataset/#model-selection","title":"Model Selection","text":"<ol> <li>Start with THETA 0.6B for prototyping</li> <li>Compare with CTM baseline</li> <li>Scale to 4B for production if needed</li> </ol>"},{"location":"examples/english-dataset/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<ol> <li>Begin with default parameters</li> <li>Adjust number of topics based on corpus</li> <li>Tune learning rate if training is unstable</li> </ol>"},{"location":"examples/english-dataset/#evaluation","title":"Evaluation","text":"<ol> <li>Review multiple metrics, not just one</li> <li>Examine visualizations for qualitative assessment</li> <li>Compare with baseline models</li> </ol>"},{"location":"examples/supervised-learning/","title":"Supervised Learning Example","text":"<p>This example demonstrates supervised topic modeling and temporal analysis.</p>"},{"location":"examples/supervised-learning/#example-1-supervised-topic-classification","title":"Example 1: Supervised Topic Classification","text":""},{"location":"examples/supervised-learning/#dataset-description","title":"Dataset Description","text":"<ul> <li>Domain: Customer reviews</li> <li>Size: 3000 documents</li> <li>Language: English</li> <li>Labels: 5 product categories</li> <li>Goal: Discover category-aligned topics</li> </ul>"},{"location":"examples/supervised-learning/#step-1-prepare-labeled-data","title":"Step 1: Prepare Labeled Data","text":"<p>CSV format with labels: <pre><code>text,label\n\"Great laptop with fast processor and long battery life\",Electronics\n\"Comfortable running shoes with good arch support\",Sports\n\"Delicious coffee beans with rich aroma\",Food\n</code></pre></p>"},{"location":"examples/supervised-learning/#step-2-preprocess-in-supervised-mode","title":"Step 2: Preprocess in Supervised Mode","text":"<pre><code>python prepare_data.py \\\n    --dataset reviews \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode supervised \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --gpu 0\n</code></pre>"},{"location":"examples/supervised-learning/#step-3-train-with-supervision","title":"Step 3: Train with Supervision","text":"<pre><code>python run_pipeline.py \\\n    --dataset reviews \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode supervised \\\n    --num_topics 15 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language en\n</code></pre>"},{"location":"examples/supervised-learning/#step-4-compare-modes","title":"Step 4: Compare Modes","text":"<p>Train both supervised and zero-shot for comparison:</p> <pre><code># Zero-shot (ignores labels)\npython run_pipeline.py \\\n    --dataset reviews \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 15 \\\n    --gpu 0\n\n# Supervised (uses labels)\npython run_pipeline.py \\\n    --dataset reviews \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode supervised \\\n    --num_topics 15 \\\n    --gpu 0\n</code></pre> <p>Results comparison:</p> Mode TD NPMI Label Alignment Zero-shot 0.85 0.41 0.62 Supervised 0.83 0.38 0.89 <p>Supervised mode achieves better label alignment with slight reduction in diversity.</p>"},{"location":"examples/supervised-learning/#step-5-topic-label-analysis","title":"Step 5: Topic-Label Analysis","text":"<pre><code>import json\nimport numpy as np\n\n# Load results\nwith open('result/0.6B/reviews/supervised/metrics/evaluation_results.json') as f:\n    results = json.load(f)\n\n# Analyze topic-label correspondence\n# Topics 0-2: Electronics\n# Topics 3-5: Sports\n# Topics 6-8: Food\n# Topics 9-11: Books\n# Topics 12-14: Clothing\n</code></pre>"},{"location":"examples/supervised-learning/#example-2-temporal-topic-evolution","title":"Example 2: Temporal Topic Evolution","text":""},{"location":"examples/supervised-learning/#dataset-description_1","title":"Dataset Description","text":"<ul> <li>Domain: Academic papers</li> <li>Size: 10000 documents</li> <li>Language: English</li> <li>Time range: 2015-2023</li> <li>Field: Machine learning</li> </ul>"},{"location":"examples/supervised-learning/#step-1-prepare-temporal-data","title":"Step 1: Prepare Temporal Data","text":"<p>CSV with year column: <pre><code>text,year\n\"Deep learning approaches for image recognition...\",2015\n\"Transformer architectures for natural language...\",2019\n\"Large language models and emergent capabilities...\",2023\n</code></pre></p>"},{"location":"examples/supervised-learning/#step-2-preprocess-with-time-information","title":"Step 2: Preprocess with Time Information","text":"<pre><code>python prepare_data.py \\\n    --dataset ml_papers \\\n    --model dtm \\\n    --vocab_size 8000 \\\n    --time_column year\n</code></pre>"},{"location":"examples/supervised-learning/#step-3-train-dtm-model","title":"Step 3: Train DTM Model","text":"<pre><code>python run_pipeline.py \\\n    --dataset ml_papers \\\n    --models dtm \\\n    --num_topics 30 \\\n    --epochs 150 \\\n    --batch_size 64 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002 \\\n    --gpu 0 \\\n    --language en\n</code></pre>"},{"location":"examples/supervised-learning/#step-4-analyze-topic-evolution","title":"Step 4: Analyze Topic Evolution","text":"<p>DTM tracks topic changes over time:</p> <p>Topic 5: Deep Learning (2015-2018) - 2015: convolutional, neural, network, classification - 2016: deep, learning, layers, training - 2017: residual, connections, skip, depth - 2018: architectures, design, efficient, mobile</p> <p>Topic 12: Attention Mechanisms (2017-2020) - 2017: attention, mechanism, sequence, encoder - 2018: self-attention, multi-head, transformer - 2019: bert, pre-training, fine-tuning, downstream - 2020: scaling, models, parameters, performance</p> <p>Topic 18: Large Language Models (2020-2023) - 2020: gpt, generation, language, model - 2021: prompting, few-shot, in-context, learning - 2022: instruction, tuning, alignment, human - 2023: emergent, capabilities, scaling, laws</p>"},{"location":"examples/supervised-learning/#step-5-visualize-trends","title":"Step 5: Visualize Trends","text":"<pre><code>python -m visualization.run_visualization \\\n    --baseline \\\n    --result_dir /root/autodl-tmp/result/baseline \\\n    --dataset ml_papers \\\n    --model dtm \\\n    --num_topics 30 \\\n    --language en \\\n    --dpi 300\n</code></pre> <p>Visualizations show: - Topic birth and death - Word probability changes over time - Topic intensity trends</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install THETA on your system.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<p>THETA requires the following system specifications:</p> <p>Operating System: - Linux (Ubuntu 18.04 or later recommended) - macOS 10.14 or later - Windows 10/11 with WSL2</p> <p>Hardware Requirements:</p> Component Minimum Recommended Python 3.8+ 3.9+ RAM 8GB 16GB+ GPU Memory 4GB (0.6B model) 12GB+ (4B model) CUDA 11.8+ 12.1+ Storage 20GB 50GB+ <p>Model-Specific GPU Requirements:</p> Model Size Parameters Embedding Dim VRAM Required Use Case 0.6B 600M 1024 ~4GB Quick experiments, limited resources 4B 4B 2560 ~12GB Balanced performance and speed 8B 8B 4096 ~24GB Best quality results"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/installation/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/CodeSoul-co/THETA.git\ncd THETA\n</code></pre>"},{"location":"getting-started/installation/#step-2-create-virtual-environment","title":"Step 2: Create Virtual Environment","text":"<p>Using conda (recommended):</p> <pre><code>conda create -n theta python=3.9\nconda activate theta\n</code></pre> <p>Using venv:</p> <pre><code>python -m venv theta-env\nsource theta-env/bin/activate  # On Linux/macOS\n# theta-env\\Scripts\\activate   # On Windows\n</code></pre>"},{"location":"getting-started/installation/#step-3-install-dependencies","title":"Step 3: Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>The installation includes the following key packages: - PyTorch (with CUDA support) - Transformers - Sentence-Transformers - Gensim - scikit-learn - NumPy, Pandas - Matplotlib, Seaborn - UMAP-learn</p>"},{"location":"getting-started/installation/#step-4-download-embedding-models","title":"Step 4: Download Embedding Models","text":"<p>Download the Qwen3-Embedding models:</p> <pre><code># For 0.6B model (recommended for first-time users)\npython scripts/download_models.py --model 0.6B\n\n# For 4B model\npython scripts/download_models.py --model 4B\n\n# For 8B model\npython scripts/download_models.py --model 8B\n</code></pre> <p>Models will be downloaded to <code>/root/embedding_models/</code> by default.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that PyTorch and CUDA are properly installed:</p> <pre><code>python -c \"import torch; print(f'PyTorch version: {torch.__version__}')\"\npython -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\npython -c \"import torch; print(f'CUDA version: {torch.version.cuda}')\"\n</code></pre> <p>Expected output: <pre><code>PyTorch version: 2.0.1+cu118\nCUDA available: True\nCUDA version: 11.8\n</code></pre></p> <p>Check THETA installation:</p> <pre><code>python -c \"from src.model import etm; print('THETA installed successfully')\"\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Tutorial - Train your first model in 5 minutes</li> <li>Data Preparation Guide - Learn about data formats</li> </ul>"},{"location":"getting-started/overview/","title":"Project Overview","text":"<p>Understanding THETA's architecture and workflow will help you use it effectively.</p>"},{"location":"getting-started/overview/#architecture-overview","title":"Architecture Overview","text":"<p>THETA consists of three main components:</p> <ol> <li>Embedding Module: Generates contextual embeddings using Qwen3-Embedding</li> <li>Topic Model: Neural variational inference for topic discovery</li> <li>Evaluation &amp; Visualization: Comprehensive assessment and presentation</li> </ol> <p>Data flow:</p> <pre><code>Raw Text \u2192 Data Cleaning \u2192 Preprocessing \u2192 Training \u2192 Evaluation \u2192 Visualization\n              \u2193              \u2193               \u2193           \u2193            \u2193\n         Cleaned CSV    Embeddings+BOW   Model Ckpt  Metrics     Figures\n</code></pre>"},{"location":"getting-started/overview/#supported-models","title":"Supported Models","text":"<p>THETA supports multiple topic modeling approaches:</p>"},{"location":"getting-started/overview/#theta-model-our-method","title":"THETA Model (Our Method)","text":"<p>Architecture: - Variational autoencoder with Qwen3-Embedding - Neural encoder for topic distribution inference - Reconstruction via topic-word distributions</p> <p>Training Modes:</p> Mode Description Use Case Requirements zero_shot Unsupervised learning No labels available Text column only supervised Label-guided learning Labels available Text + label columns unsupervised Unsupervised (ignores labels) Compare with zero_shot Text column only <p>Model Sizes:</p> <p>All three sizes share the same architecture but differ in embedding quality: - 0.6B: Fastest, suitable for development and testing - 4B: Balanced performance for production use - 8B: Best quality for research and high-stakes applications</p>"},{"location":"getting-started/overview/#baseline-models","title":"Baseline Models","text":"<p>LDA (Latent Dirichlet Allocation) - Classic probabilistic topic model - No neural components - Fast and interpretable</p> <p>ETM (Embedded Topic Model) - Uses Word2Vec embeddings - Neural topic model - Better than LDA, faster than THETA</p> <p>CTM (Contextualized Topic Model) - Uses SBERT embeddings - Contextualized representations - Good balance of quality and speed</p> <p>DTM (Dynamic Topic Model) - Temporal topic modeling - Tracks topic evolution over time - Requires timestamp information</p>"},{"location":"getting-started/overview/#directory-structure","title":"Directory Structure","text":"<p>THETA organizes files in the following structure:</p>"},{"location":"getting-started/overview/#project-directory","title":"Project Directory","text":"<pre><code>/root/autodl-tmp/ETM/\n\u251c\u2500\u2500 main.py                   # THETA training script\n\u251c\u2500\u2500 run_pipeline.py           # Unified entry point\n\u251c\u2500\u2500 prepare_data.py           # Data preprocessing\n\u251c\u2500\u2500 config.py                 # Configuration\n\u251c\u2500\u2500 requirements.txt          # Dependencies\n\u251c\u2500\u2500 dataclean/               # Data cleaning module\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 bow/                 # BOW generation\n\u2502   \u251c\u2500\u2500 model/               # Model definitions\n\u2502   \u2502   \u251c\u2500\u2500 etm.py          # THETA/ETM model\n\u2502   \u2502   \u251c\u2500\u2500 lda.py          # LDA model\n\u2502   \u2502   \u251c\u2500\u2500 ctm.py          # CTM model\n\u2502   \u2502   \u2514\u2500\u2500 baseline_trainer.py\n\u2502   \u251c\u2500\u2500 evaluation/          # Evaluation metrics\n\u2502   \u2502   \u251c\u2500\u2500 topic_metrics.py\n\u2502   \u2502   \u2514\u2500\u2500 unified_evaluator.py\n\u2502   \u251c\u2500\u2500 visualization/       # Visualization\n\u2502   \u2502   \u251c\u2500\u2500 run_visualization.py\n\u2502   \u2502   \u251c\u2500\u2500 topic_visualizer.py\n\u2502   \u2502   \u2514\u2500\u2500 visualization_generator.py\n\u2502   \u2514\u2500\u2500 utils/               # Utilities\n\u2502       \u2514\u2500\u2500 result_manager.py\n\u2514\u2500\u2500 scripts/\n    \u2514\u2500\u2500 download_models.py\n</code></pre>"},{"location":"getting-started/overview/#data-directory","title":"Data Directory","text":"<pre><code>/root/autodl-tmp/data/\n\u2514\u2500\u2500 {dataset_name}/\n    \u2514\u2500\u2500 {dataset_name}_cleaned.csv\n</code></pre>"},{"location":"getting-started/overview/#results-directory","title":"Results Directory","text":"<pre><code>/root/autodl-tmp/result/\n\u251c\u2500\u2500 0.6B/                    # THETA 0.6B results\n\u2502   \u2514\u2500\u2500 {dataset_name}/\n\u2502       \u251c\u2500\u2500 bow/             # Shared by all modes\n\u2502       \u251c\u2500\u2500 zero_shot/       # Zero-shot results\n\u2502       \u2502   \u251c\u2500\u2500 checkpoints/\n\u2502       \u2502   \u251c\u2500\u2500 metrics/\n\u2502       \u2502   \u2514\u2500\u2500 visualizations/\n\u2502       \u251c\u2500\u2500 supervised/      # Supervised results\n\u2502       \u2514\u2500\u2500 unsupervised/    # Unsupervised results\n\u251c\u2500\u2500 4B/                      # THETA 4B results\n\u251c\u2500\u2500 8B/                      # THETA 8B results\n\u2514\u2500\u2500 baseline/                # Baseline results\n    \u2514\u2500\u2500 {dataset_name}/\n        \u251c\u2500\u2500 bow/\n        \u251c\u2500\u2500 lda/\n        \u2502   \u2514\u2500\u2500 K20/        # 20 topics\n        \u251c\u2500\u2500 etm/\n        \u251c\u2500\u2500 ctm/\n        \u2514\u2500\u2500 dtm/\n</code></pre>"},{"location":"getting-started/overview/#embedding-models-directory","title":"Embedding Models Directory","text":"<pre><code>/root/embedding_models/\n\u251c\u2500\u2500 qwen3_embedding_0.6B/\n\u251c\u2500\u2500 qwen3_embedding_4B/\n\u2514\u2500\u2500 qwen3_embedding_8B/\n</code></pre>"},{"location":"getting-started/overview/#workflow-summary","title":"Workflow Summary","text":"<p>The typical THETA workflow consists of four stages:</p> <p>Stage 1: Data Preparation 1. Collect raw text data 2. Clean and format as CSV 3. Ensure proper column names</p> <p>Stage 2: Preprocessing 1. Run <code>prepare_data.py</code> to generate embeddings 2. Create bag-of-words representations 3. Build vocabulary 4. Save preprocessed files</p> <p>Stage 3: Training 1. Run <code>run_pipeline.py</code> to train model 2. Model trains with early stopping 3. Automatic evaluation on multiple metrics 4. Automatic visualization generation</p> <p>Stage 4: Analysis 1. Review evaluation metrics 2. Examine visualizations 3. Analyze discovered topics 4. Compare with baseline models</p>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<p>Now that you understand the architecture, you can:</p> <ul> <li>Explore the User Guide for detailed documentation on each component</li> <li>Try different training modes (supervised, unsupervised)</li> <li>Experiment with different model sizes (4B, 8B)</li> <li>Learn about hyperparameter tuning in the Advanced Usage section</li> <li>Compare THETA with baseline models (LDA, ETM, CTM)</li> <li>Process Chinese text data with specialized pipelines</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This tutorial demonstrates how to train a THETA model on your dataset in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":"<p>Create a CSV file with your text data. The CSV must contain a column with text content.</p> <p>Example CSV format:</p> <pre><code>text\n\"First document discussing climate change and global warming.\"\n\"Second document about renewable energy sources.\"\n\"Third document on environmental policy and regulations.\"\n</code></pre> <p>Required columns:</p> Column Name Type Required Description text / content / cleaned_content / clean_text string Yes Text content for topic modeling label / category string/int No Labels for supervised mode year / timestamp / date int/string No Timestamp for DTM model <p>Save your CSV file to the data directory:</p> <pre><code>mkdir -p /root/autodl-tmp/data/my_dataset\ncp your_data.csv /root/autodl-tmp/data/my_dataset/my_dataset_cleaned.csv\n</code></pre> <p>Note: The CSV filename must follow the pattern <code>{dataset_name}_cleaned.csv</code>.</p>"},{"location":"getting-started/quickstart/#step-2-preprocess-data","title":"Step 2: Preprocess Data","text":"<p>Generate embeddings and bag-of-words representations:</p> <pre><code>cd /root/autodl-tmp/ETM\n\npython prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --max_length 512 \\\n    --gpu 0\n</code></pre> <p>What this does: 1. Loads your CSV file 2. Generates Qwen embeddings for all documents 3. Creates bag-of-words representations 4. Builds vocabulary 5. Saves preprocessed data to <code>/root/autodl-tmp/result/0.6B/my_dataset/bow/</code></p> <p>Expected output: <pre><code>Loading dataset: my_dataset\nProcessing 1000 documents...\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:45&lt;00:00, 1.41s/batch]\nBuilding vocabulary (size=5000)...\nSaving preprocessed data...\nDone! Files saved to /root/autodl-tmp/result/0.6B/my_dataset/bow/\n</code></pre></p> <p>Verify that data files were created:</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --check-only\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-train-the-model","title":"Step 3: Train the Model","text":"<p>Train a THETA model with 20 topics:</p> <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002 \\\n    --kl_start 0.0 \\\n    --kl_end 1.0 \\\n    --kl_warmup 50 \\\n    --patience 10 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>Training parameters explained:</p> Parameter Value Description <code>--num_topics</code> 20 Number of topics to discover <code>--epochs</code> 100 Maximum training epochs <code>--batch_size</code> 64 Batch size for training <code>--hidden_dim</code> 512 Hidden dimension of encoder <code>--learning_rate</code> 0.002 Learning rate for optimizer <code>--kl_start</code> 0.0 Initial KL annealing weight <code>--kl_end</code> 1.0 Final KL annealing weight <code>--kl_warmup</code> 50 Epochs for KL warmup <code>--patience</code> 10 Early stopping patience <p>Training progress: <pre><code>Epoch 1/100: Loss=245.32, ELBO=-243.12, KL=2.20\nEpoch 10/100: Loss=156.78, ELBO=-154.56, KL=2.22\nEpoch 20/100: Loss=142.35, ELBO=-139.87, KL=2.48\n...\nEpoch 65/100: Loss=128.45, ELBO=-125.23, KL=3.22\nEarly stopping triggered at epoch 65\nTraining completed in 23.5 minutes\n</code></pre></p> <p>The training process automatically: 1. Trains the model 2. Evaluates on multiple metrics 3. Generates visualizations 4. Saves all results</p>"},{"location":"getting-started/quickstart/#step-4-view-results","title":"Step 4: View Results","text":"<p>After training, results are saved in:</p> <pre><code>/root/autodl-tmp/result/0.6B/my_dataset/zero_shot/\n\u251c\u2500\u2500 checkpoints/\n\u2502   \u2514\u2500\u2500 best_model.pt\n\u251c\u2500\u2500 metrics/\n\u2502   \u2514\u2500\u2500 evaluation_results.json\n\u2514\u2500\u2500 visualizations/\n    \u251c\u2500\u2500 topic_words_bars.png\n    \u251c\u2500\u2500 topic_similarity.png\n    \u251c\u2500\u2500 doc_topic_umap.png\n    \u251c\u2500\u2500 topic_wordclouds.png\n    \u251c\u2500\u2500 metrics.png\n    \u2514\u2500\u2500 pyldavis.html\n</code></pre> <p>View evaluation metrics:</p> <pre><code>cat /root/autodl-tmp/result/0.6B/my_dataset/zero_shot/metrics/evaluation_results.json\n</code></pre> <p>Example output: <pre><code>{\n  \"TD\": 0.89,\n  \"iRBO\": 0.76,\n  \"NPMI\": 0.42,\n  \"C_V\": 0.65,\n  \"UMass\": -2.34,\n  \"Exclusivity\": 0.82,\n  \"PPL\": 145.23\n}\n</code></pre></p> <p>View visualizations:</p> <p>Open the visualization files in your browser or image viewer: - <code>topic_words_bars.png</code>: Bar charts showing top words for each topic - <code>topic_similarity.png</code>: Heatmap of topic similarities - <code>doc_topic_umap.png</code>: UMAP projection of documents in topic space - <code>pyldavis.html</code>: Interactive visualization (open in browser)</p>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>User Guide - Complete workflow documentation</li> <li>Advanced Usage - Advanced features</li> <li>Examples - Real-world use cases</li> </ul>"},{"location":"models/baselines/","title":"Baseline Models","text":"<p>THETA includes several baseline models for comparison.</p>"},{"location":"models/baselines/#lda-latent-dirichlet-allocation","title":"LDA (Latent Dirichlet Allocation)","text":"<p>Classic probabilistic topic model using Dirichlet-multinomial distributions.</p> <p>Architecture: - No neural components - Dirichlet priors on topic and word distributions - Inference via Gibbs sampling or variational inference</p> <p>Strengths: - Well-established theoretical foundation - Interpretable probabilistic framework - Efficient on CPU - Deterministic given random seed</p> <p>Limitations: - No semantic word relationships - Bag-of-words assumption - Performance plateaus on large vocabularies</p> <p>Training: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models lda \\\n    --num_topics 20 \\\n    --epochs 100\n</code></pre></p>"},{"location":"models/baselines/#etm-embedded-topic-model","title":"ETM (Embedded Topic Model)","text":"<p>Neural topic model using Word2Vec embeddings.</p> <p>Architecture: - VAE framework similar to THETA - Word2Vec embeddings (300 dimensions) - Topic embeddings in same space as word embeddings</p> <p>Strengths: - Captures semantic relationships - More coherent topics than LDA - Efficient training with GPU - Original ETM implementation</p> <p>Limitations: - Word2Vec limited to static embeddings - 300-dimensional embeddings less expressive than Qwen - Requires pre-trained Word2Vec model</p> <p>Training: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models etm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002\n</code></pre></p>"},{"location":"models/baselines/#ctm-contextualized-topic-model","title":"CTM (Contextualized Topic Model)","text":"<p>Neural topic model using SBERT contextualized embeddings.</p> <p>Architecture: - VAE framework with SBERT encoder - SBERT embeddings (768 dimensions) - Contextualized document representations</p> <p>Strengths: - Contextualized semantic representations - Better than ETM on most metrics - Faster than large THETA models - Widely used in recent research</p> <p>Limitations: - SBERT embeddings fixed at 768 dimensions - Less powerful than Qwen 4B/8B models - Requires SBERT model download</p> <p>Training: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models ctm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002\n</code></pre></p>"},{"location":"models/baselines/#dtm-dynamic-topic-model","title":"DTM (Dynamic Topic Model)","text":"<p>Temporal extension of CTM for tracking topic evolution over time.</p> <p>Architecture: - Based on CTM architecture - Additional temporal dynamics layer - Models topic transitions between time slices</p> <p>Strengths: - Captures temporal dynamics - Reveals emerging and declining topics - Useful for trend analysis - Handles variable time slice sizes</p> <p>Limitations: - Requires time column in data - More parameters to estimate - Longer training time - Needs sufficient documents per time slice</p> <p>Training: <pre><code>python run_pipeline.py \\\n    --dataset temporal_dataset \\\n    --models dtm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002\n</code></pre></p> <p>Data requirements: - CSV must contain time column (year, timestamp, or date) - Preprocessing must specify time column via <code>--time_column</code></p>"},{"location":"models/comparison/","title":"Model Comparison","text":"<p>Comprehensive comparison of all models supported by THETA.</p>"},{"location":"models/comparison/#performance-comparison","title":"Performance Comparison","text":"<p>Typical performance on benchmark datasets:</p> Model TD NPMI C_V PPL Speed VRAM LDA 0.75 0.25 0.45 180 Fast 0GB ETM 0.82 0.32 0.52 165 Medium 4GB CTM 0.85 0.38 0.58 155 Medium 6GB THETA-0.6B 0.88 0.42 0.64 145 Medium 8GB THETA-4B 0.91 0.48 0.69 138 Slow 16GB THETA-8B 0.93 0.52 0.72 132 Slowest 28GB <p>Values are approximate and vary by dataset. Higher is better for TD, NPMI, C_V. Lower is better for PPL.</p>"},{"location":"models/comparison/#selection-guidelines","title":"Selection Guidelines","text":"<p>Use LDA when: - Need fast baseline results - Interpretability is critical - No GPU available - Computing topic distributions for new documents frequently</p> <p>Use ETM when: - Want better performance than LDA - Have GPU available - Need moderate computational budget - Comparing against original ETM papers</p> <p>Use CTM when: - Need contextualized understanding - Want good balance of quality and speed - Following recent topic modeling literature - Working with standard-size corpora</p> <p>Use DTM when: - Analyzing temporal dynamics - Have time-stamped documents - Studying topic evolution - Investigating emerging trends</p> <p>Use THETA-0.6B when: - Need better quality than CTM - Have 8-12GB VRAM available - Rapid experimentation required</p> <p>Use THETA-4B when: - Need high-quality results - Have 16-20GB VRAM available - Production deployment</p> <p>Use THETA-8B when: - Need highest possible quality - Have 24-32GB VRAM available - Critical applications</p>"},{"location":"models/comparison/#computational-requirements","title":"Computational Requirements","text":"<p>Training time comparison on 10K document corpus:</p> Model CPU Time GPU Time VRAM Storage LDA 15 min N/A 0GB 100MB ETM N/A 20 min 4GB 500MB CTM N/A 25 min 6GB 800MB THETA-0.6B N/A 30 min 8GB 2GB THETA-4B N/A 50 min 16GB 6GB THETA-8B N/A 90 min 28GB 12GB <p>Times assume single GPU (V100 or A100).</p>"},{"location":"models/comparison/#embedding-comparison","title":"Embedding Comparison","text":"Model Embedding Dimension Contextual Pre-trained LDA None N/A No N/A ETM Word2Vec 300 No Yes CTM SBERT 768 Yes Yes THETA-0.6B Qwen3 1024 Yes Yes THETA-4B Qwen3 2560 Yes Yes THETA-8B Qwen3 4096 Yes Yes"},{"location":"models/comparison/#model-selection-workflow","title":"Model Selection Workflow","text":""},{"location":"models/comparison/#step-1-determine-requirements","title":"Step 1: Determine Requirements","text":"<p>Consider: - Dataset size (number of documents) - Available computational resources (GPU memory) - Time constraints - Quality requirements (research vs prototyping)</p>"},{"location":"models/comparison/#step-2-choose-initial-model","title":"Step 2: Choose Initial Model","text":"<p>Default recommendations: - Prototyping: THETA-0.6B or CTM - Production: THETA-4B - Research: THETA-8B - Quick baseline: LDA - Temporal analysis: DTM</p>"},{"location":"models/comparison/#step-3-evaluate-and-compare","title":"Step 3: Evaluate and Compare","text":"<p>Train multiple models: <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models lda,etm,ctm,theta \\\n    --model_size 0.6B \\\n    --num_topics 20\n</code></pre></p>"},{"location":"models/comparison/#step-4-scale-up-if-needed","title":"Step 4: Scale Up If Needed","text":"<ul> <li>THETA-0.6B \u2192 THETA-4B: Significant quality improvement</li> <li>THETA-4B \u2192 THETA-8B: Marginal quality improvement</li> <li>Consider collecting more data before scaling model size</li> </ul>"},{"location":"models/theta/","title":"THETA Model","text":"<p>THETA is a neural topic model that combines variational autoencoders with Qwen3-Embedding representations.</p>"},{"location":"models/theta/#architecture","title":"Architecture","text":"<p>The model consists of three main components:</p> <p>Encoder Network - Input: Qwen embeddings (dimension 1024/2560/4096 depending on model size) - Architecture: Multi-layer perceptron with configurable hidden dimension - Output: Parameters of variational posterior q(\u03b8|x)   - Mean \u03bc \u2208 R^K (K = number of topics)   - Log-variance log \u03c3^2 \u2208 R^K</p> <p>Reparameterization - Sample topic distribution using reparameterization trick - \u03b8 = \u03bc + \u03c3 \u2299 \u03b5, where \u03b5 ~ N(0, I) - Enables gradient-based training through stochastic sampling</p> <p>Decoder Network - Topic-word matrix \u03b2 \u2208 R^(K\u00d7V) (V = vocabulary size) - Reconstruction: p(w|\u03b8) = softmax(\u03b8^T \u03b2) - Loss: Negative ELBO = -E_q[log p(w|\u03b8)] + KL[q(\u03b8|x) || p(\u03b8)]</p>"},{"location":"models/theta/#training-objective","title":"Training Objective","text":"<p>The model maximizes the evidence lower bound (ELBO):</p> <pre><code>ELBO = E_q(\u03b8|x)[log p(w|\u03b8)] - KL[q(\u03b8|x) || p(\u03b8)]\n</code></pre> <p>Components: - Reconstruction term: Expected log-likelihood of observed words - KL divergence: Regularization toward prior p(\u03b8) = Dir(\u03b1)</p> <p>KL annealing is applied to prevent posterior collapse: <pre><code>Loss = -Reconstruction + \u03b2_t * KL\n</code></pre> where \u03b2_t increases from 0 to 1 during warmup period.</p>"},{"location":"models/theta/#model-specifications","title":"Model Specifications","text":"<p>0.6B Model</p> Property Value Parameters 600M Embedding Dimension 1024 VRAM Requirement ~4GB Processing Speed ~1000 docs/min Batch Size (preprocessing) 32 Batch Size (training) 64 <p>Characteristics: - Fastest processing speed - Suitable for development and iteration - Good quality on most datasets - Recommended starting point</p> <p>4B Model</p> Property Value Parameters 4B Embedding Dimension 2560 VRAM Requirement ~12GB Processing Speed ~400 docs/min Batch Size (preprocessing) 16 Batch Size (training) 32 <p>Characteristics: - Balanced performance and cost - Better semantic understanding than 0.6B - Suitable for production deployments - Recommended for final results</p> <p>8B Model</p> Property Value Parameters 8B Embedding Dimension 4096 VRAM Requirement ~24GB Processing Speed ~200 docs/min Batch Size (preprocessing) 8 Batch Size (training) 16 <p>Characteristics: - Highest quality embeddings - Best performance on all metrics - Requires high-end GPU (A100, H100) - Recommended for research and critical applications</p>"},{"location":"models/theta/#training-modes","title":"Training Modes","text":"<p>zero_shot Mode</p> <p>Standard unsupervised topic modeling: - No label information used - Topics emerge purely from text patterns - Default choice when labels are unavailable</p> <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20\n</code></pre> <p>supervised Mode</p> <p>Label-guided topic discovery: - Incorporates label information during training - Topics align with provided categories - Requires label column in CSV</p> <p>The model adds a classification objective: <pre><code>Loss = -ELBO + \u03bb * CrossEntropy(y_pred, y_true)\n</code></pre></p> <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode supervised \\\n    --num_topics 20\n</code></pre> <p>unsupervised Mode</p> <p>Explicit unsupervised learning: - Similar to zero_shot but explicitly ignores labels if present - Used for comparison experiments on labeled data - Useful for ablation studies</p> <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode unsupervised \\\n    --num_topics 20\n</code></pre>"},{"location":"models/theta/#hyperparameter-guidelines","title":"Hyperparameter Guidelines","text":"<p>Number of Topics</p> <p>Selection depends on corpus characteristics: - Small corpus (&lt; 1K documents): 10-20 topics - Medium corpus (1K-10K documents): 20-50 topics - Large corpus (&gt; 10K documents): 50-100 topics</p> <p>Hidden Dimension</p> <p>Controls encoder capacity: - 256: Minimal capacity, faster training - 512: Default choice, works for most cases - 768-1024: Higher capacity for complex corpora</p> <p>Learning Rate</p> <p>Affects convergence speed and stability: - 0.001: Conservative, stable convergence - 0.002: Default, balanced performance - 0.005: Aggressive, faster but less stable</p> <p>KL Annealing</p> <p>Standard schedule: - Start: 0.0 (no KL penalty) - End: 1.0 (full KL penalty) - Warmup: 50 epochs (gradual increase)</p>"},{"location":"models/theta/#implementation-details","title":"Implementation Details","text":""},{"location":"models/theta/#checkpoint-management","title":"Checkpoint Management","text":"<p>Model checkpoints are saved during training: - <code>best_model.pt</code>: Best model by validation loss - <code>last_model.pt</code>: Final epoch model - <code>training_history.json</code>: Loss curves and metrics</p> <p>Load checkpoint for inference: <pre><code>from src.model import etm\nmodel = etm.THETA(num_topics=20, vocab_size=5000)\nmodel.load_state_dict(torch.load('best_model.pt'))\n</code></pre></p>"},{"location":"models/theta/#memory-management","title":"Memory Management","text":"<p>GPU memory usage scales with: - Batch size (linear scaling) - Embedding dimension (linear scaling) - Vocabulary size (linear scaling) - Hidden dimension (linear scaling)</p> <p>Reduce memory usage by: - Decreasing batch size - Using smaller model (0.6B instead of 4B) - Reducing vocabulary size - Reducing hidden dimension</p>"},{"location":"models/theta/#reproducibility","title":"Reproducibility","text":"<p>Set random seeds for reproducible results: <pre><code>import torch\nimport numpy as np\nimport random\n\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.cuda.manual_seed_all(seed)\n</code></pre></p> <p>Deterministic operations: <pre><code>torch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n</code></pre></p> <p>Note: Some operations are non-deterministic on GPU even with seeding.</p>"},{"location":"user-guide/data-preparation/","title":"Data Preparation","text":"<p>This guide covers data format requirements and cleaning procedures.</p>"},{"location":"user-guide/data-preparation/#data-format-requirements","title":"Data Format Requirements","text":"<p>THETA accepts CSV files with specific column requirements. The preprocessing pipeline recognizes several standard column names for text content.</p> <p>Accepted text column names: - <code>text</code> - <code>content</code> - <code>cleaned_content</code> - <code>clean_text</code></p> <p>Optional columns: - <code>label</code> or <code>category</code> - Required for supervised mode - <code>year</code>, <code>timestamp</code>, or <code>date</code> - Required for DTM (temporal analysis)</p> <p>Example CSV structure:</p> <pre><code>text,label,year\n\"Document about renewable energy and solar panels.\",Environment,2020\n\"Article discussing machine learning applications.\",Technology,2021\n\"Policy paper on healthcare reform.\",Healthcare,2022\n</code></pre>"},{"location":"user-guide/data-preparation/#data-cleaning","title":"Data Cleaning","text":"<p>Raw text often contains noise that degrades topic quality. The data cleaning module handles common issues in both English and Chinese text.</p>"},{"location":"user-guide/data-preparation/#english-data-cleaning","title":"English Data Cleaning","text":"<pre><code>cd /root/autodl-tmp/ETM\n\npython -m dataclean.main \\\n    --input /root/autodl-tmp/data/raw_data.csv \\\n    --output /root/autodl-tmp/data/cleaned_data.csv \\\n    --language english\n</code></pre> <p>The cleaning process removes: - HTML tags and markup - URLs and email addresses - Special characters and symbols - Extra whitespace - Non-printable characters</p>"},{"location":"user-guide/data-preparation/#chinese-data-cleaning","title":"Chinese Data Cleaning","text":"<p>Chinese text requires specialized processing for proper segmentation and cleaning.</p> <pre><code>python -m dataclean.main \\\n    --input /root/autodl-tmp/data/raw_data.csv \\\n    --output /root/autodl-tmp/data/cleaned_data.csv \\\n    --language chinese\n</code></pre> <p>Additional steps for Chinese: - Removes traditional punctuation marks - Handles full-width and half-width characters - Preserves Chinese word boundaries</p>"},{"location":"user-guide/data-preparation/#batch-cleaning","title":"Batch Cleaning","text":"<p>Process multiple files in a directory:</p> <pre><code>python -m dataclean.main \\\n    --input /root/autodl-tmp/data/raw/ \\\n    --output /root/autodl-tmp/data/cleaned/ \\\n    --language english\n</code></pre> <p>All CSV files in the input directory will be processed and saved to the output directory with the same filenames.</p>"},{"location":"user-guide/evaluation/","title":"Evaluation","text":"<p>Training automatically runs evaluation using multiple metrics. Results are saved in JSON format.</p>"},{"location":"user-guide/evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Topic Diversity (TD) - Range: 0-1 - Higher is better - Measures uniqueness of topics - Computed as percentage of unique words in top-N words across all topics</p> <p>Inverse Rank-Biased Overlap (iRBO) - Range: 0-1 - Higher is better - Measures topic distinctiveness - Lower values indicate redundant or overlapping topics</p> <p>Normalized PMI (NPMI) - Range: -1 to 1 - Higher is better - Measures semantic coherence of topic words - Based on pointwise mutual information in external corpus</p> <p>C_V Coherence - Range: 0-1 - Higher is better - Alternative coherence measure - Based on sliding window co-occurrence</p> <p>UMass Coherence - Range: Negative values - Closer to 0 is better - Classic coherence metric - Based on document co-occurrence</p> <p>Exclusivity - Range: 0-1 - Higher is better - Measures topic specificity - Computed using FREX score</p> <p>Perplexity (PPL) - Range: Positive values - Lower is better - Measures model fit on held-out data - Standard evaluation for probabilistic models</p>"},{"location":"user-guide/evaluation/#viewing-evaluation-results","title":"Viewing Evaluation Results","text":"<p>Results are saved in the metrics directory:</p> <pre><code>cat /root/autodl-tmp/result/0.6B/my_dataset/zero_shot/metrics/evaluation_results.json\n</code></pre> <p>Example output: <pre><code>{\n  \"TD\": 0.891,\n  \"iRBO\": 0.762,\n  \"NPMI\": 0.418,\n  \"C_V\": 0.654,\n  \"UMass\": -2.341,\n  \"Exclusivity\": 0.823,\n  \"PPL\": 145.23,\n  \"training_time\": 1425.6,\n  \"num_topics\": 20,\n  \"num_documents\": 5000\n}\n</code></pre></p>"},{"location":"user-guide/evaluation/#running-evaluation-separately","title":"Running Evaluation Separately","text":"<p>Skip training and only evaluate existing models:</p> <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --skip-train \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>This loads the trained model from checkpoints and recomputes all metrics.</p>"},{"location":"user-guide/evaluation/#comparing-multiple-models","title":"Comparing Multiple Models","text":"<p>Evaluate all baseline models:</p> <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models lda,etm,ctm \\\n    --num_topics 20 \\\n    --skip-train \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>Results for each model are saved separately. Use the metrics files to construct comparison tables.</p>"},{"location":"user-guide/preprocessing/","title":"Data Preprocessing","text":"<p>Preprocessing converts cleaned text into numerical representations required for training. This stage generates embeddings using Qwen models and constructs bag-of-words representations.</p>"},{"location":"user-guide/preprocessing/#theta-model-preprocessing","title":"THETA Model Preprocessing","text":""},{"location":"user-guide/preprocessing/#basic-preprocessing","title":"Basic Preprocessing","text":"<p>For a dataset named <code>my_dataset</code> with a cleaned CSV file:</p> <pre><code>cd /root/autodl-tmp/ETM\n\npython prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --max_length 512 \\\n    --gpu 0\n</code></pre> <p>This command: 1. Loads the CSV from <code>/root/autodl-tmp/data/my_dataset/my_dataset_cleaned.csv</code> 2. Generates Qwen embeddings (dimension 1024 for 0.6B model) 3. Constructs bag-of-words with vocabulary size 5000 4. Saves output to <code>/root/autodl-tmp/result/0.6B/my_dataset/bow/</code></p>"},{"location":"user-guide/preprocessing/#model-size-selection","title":"Model Size Selection","text":"<p>0.6B Model - Default choice for most use cases</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --gpu 0\n</code></pre> <p>Processing speed: ~1000 documents per minute on single GPU Memory requirement: 4GB VRAM</p> <p>4B Model - Better quality at moderate cost</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 4B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 16 \\\n    --gpu 0\n</code></pre> <p>Processing speed: ~400 documents per minute Memory requirement: 12GB VRAM Batch size reduced to 16 due to larger embeddings (dimension 2560)</p> <p>8B Model - Highest quality</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 8B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 8 \\\n    --gpu 0\n</code></pre> <p>Processing speed: ~200 documents per minute Memory requirement: 24GB VRAM Batch size reduced to 8 due to large embeddings (dimension 4096)</p>"},{"location":"user-guide/preprocessing/#training-mode-selection","title":"Training Mode Selection","text":"<p>zero_shot mode - Standard unsupervised learning</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --gpu 0\n</code></pre> <p>Use when: No labels available or labels should be ignored</p> <p>supervised mode - Label-guided learning</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode supervised \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --gpu 0\n</code></pre> <p>Use when: CSV contains <code>label</code> or <code>category</code> column The model will incorporate label information during training</p> <p>unsupervised mode - Explicit unsupervised mode</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode unsupervised \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --gpu 0\n</code></pre> <p>Use when: Comparing with zero_shot mode on labeled data while ignoring labels</p>"},{"location":"user-guide/preprocessing/#vocabulary-configuration","title":"Vocabulary Configuration","text":"<p>Vocabulary size affects model capacity and training speed. Larger vocabularies capture more terms but increase computation.</p> Vocabulary Size Appropriate For 3000-5000 Small corpora, domain-specific text, faster training 5000-8000 General purpose, default setting 8000-15000 Large diverse corpora, capturing rare terms"},{"location":"user-guide/preprocessing/#sequence-length-configuration","title":"Sequence Length Configuration","text":"<p>The <code>max_length</code> parameter controls input truncation for embedding generation.</p> Max Length Appropriate For 256 Short documents (tweets, reviews), faster processing 512 Medium documents (news articles), default setting 1024 Long documents (papers, reports), requires more VRAM"},{"location":"user-guide/preprocessing/#combined-cleaning-and-preprocessing","title":"Combined Cleaning and Preprocessing","text":"<p>Process raw data in a single step:</p> <p>English data: <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --max_length 512 \\\n    --clean \\\n    --raw-input /root/autodl-tmp/data/my_dataset/raw_data.csv \\\n    --language english \\\n    --gpu 0\n</code></pre></p> <p>Chinese data: <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --vocab_size 5000 \\\n    --batch_size 32 \\\n    --max_length 512 \\\n    --clean \\\n    --raw-input /root/autodl-tmp/data/my_dataset/raw_data.csv \\\n    --language chinese \\\n    --gpu 0\n</code></pre></p> <p>The <code>--clean</code> flag triggers automatic cleaning before preprocessing. The cleaned CSV is saved as <code>{dataset}_cleaned.csv</code> in the dataset directory.</p>"},{"location":"user-guide/preprocessing/#verifying-preprocessed-data","title":"Verifying Preprocessed Data","text":"<p>Check that all required files were generated:</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --check-only\n</code></pre> <p>Expected output: <pre><code>Checking preprocessed files for dataset: my_dataset\n\u2713 BOW data: /root/autodl-tmp/result/0.6B/my_dataset/bow/\n\u2713 Embeddings: qwen_embeddings_zeroshot.npy (1024 dims)\n\u2713 Vocabulary: vocab.pkl (5000 words)\n\u2713 Document indices: doc_indices.npy\nAll required files present.\n</code></pre></p>"},{"location":"user-guide/preprocessing/#baseline-model-preprocessing","title":"Baseline Model Preprocessing","text":"<p>Baseline models (LDA, ETM, CTM) use different preprocessing pipelines that do not require Qwen embeddings.</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model baseline \\\n    --vocab_size 5000\n</code></pre> <p>This generates: - Bag-of-words representations - TF-IDF vectors (for CTM) - Word2Vec embeddings (for ETM) - Document-term matrix (for LDA)</p> <p>Output location: <code>/root/autodl-tmp/result/baseline/my_dataset/bow/</code></p>"},{"location":"user-guide/preprocessing/#dtm-model-preprocessing","title":"DTM Model Preprocessing","text":"<p>DTM requires temporal information in the CSV. Specify the time column name:</p> <pre><code>python prepare_data.py \\\n    --dataset my_dataset \\\n    --model dtm \\\n    --vocab_size 5000 \\\n    --time_column year\n</code></pre> <p>The time column can be named <code>year</code>, <code>timestamp</code>, or <code>date</code>. Documents are automatically grouped by time slice for temporal modeling.</p>"},{"location":"user-guide/training/","title":"Training Models","text":"<p>This guide covers training THETA and baseline models with various configurations.</p>"},{"location":"user-guide/training/#theta-model-training","title":"THETA Model Training","text":""},{"location":"user-guide/training/#basic-training","title":"Basic Training","text":"<p>Train a THETA model with default hyperparameters:</p> <pre><code>cd /root/autodl-tmp/ETM\n\npython run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002 \\\n    --kl_start 0.0 \\\n    --kl_end 1.0 \\\n    --kl_warmup 50 \\\n    --patience 10 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>Training typically completes in 20-40 minutes depending on dataset size and hardware.</p>"},{"location":"user-guide/training/#topic-number-selection","title":"Topic Number Selection","text":"<p>The number of topics is a key hyperparameter that affects granularity:</p> Topics Appropriate For 10-15 Small corpora, broad categories, high-level overview 20-30 Medium corpora, balanced granularity, default choice 40-100 Large diverse corpora, fine-grained analysis"},{"location":"user-guide/training/#learning-rate-tuning","title":"Learning Rate Tuning","text":"Learning Rate Use When 0.001 Training is unstable, loss oscillates 0.002 Default choice for most datasets 0.005 Training is too slow, need faster convergence"},{"location":"user-guide/training/#kl-annealing-configuration","title":"KL Annealing Configuration","text":"<p>KL annealing gradually increases the KL divergence weight during training to prevent posterior collapse.</p> <p>Standard KL annealing: Weight increases linearly from 0.0 to 1.0 over 50 epochs.</p> <p>Slow KL annealing: <code>--kl_warmup 80</code> \u2014 Longer warmup period helps prevent early posterior collapse.</p> <p>Partial KL annealing: <code>--kl_start 0.1 --kl_end 0.9 --kl_warmup 30</code> \u2014 Starts with non-zero weight and stops before full weight.</p>"},{"location":"user-guide/training/#hidden-dimension-configuration","title":"Hidden Dimension Configuration","text":"Hidden Dim Use When 256 Small datasets, faster training, limited VRAM 512 Default choice for most datasets 768-1024 Large complex datasets, sufficient VRAM available"},{"location":"user-guide/training/#early-stopping","title":"Early Stopping","text":"<p>Early stopping prevents overfitting by monitoring validation performance:</p> <ul> <li>Default: <code>--patience 10</code> \u2014 Stops if no improvement after 10 epochs</li> <li>Disabled: <code>--no_early_stopping</code> \u2014 Trains for all specified epochs</li> </ul>"},{"location":"user-guide/training/#chinese-data-training","title":"Chinese Data Training","text":"<pre><code>python run_pipeline.py \\\n    --dataset chinese_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language zh\n</code></pre> <p>The language parameter affects visualization rendering (fonts, layout) but does not change the training algorithm.</p>"},{"location":"user-guide/training/#supervised-training","title":"Supervised Training","text":"<p>For datasets with labels:</p> <pre><code>python run_pipeline.py \\\n    --dataset labeled_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode supervised \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>The model incorporates label information to guide topic discovery.</p>"},{"location":"user-guide/training/#baseline-model-training","title":"Baseline Model Training","text":""},{"location":"user-guide/training/#lda-training","title":"LDA Training","text":"<pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models lda \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>LDA uses Gibbs sampling and does not utilize GPU acceleration.</p>"},{"location":"user-guide/training/#etm-training","title":"ETM Training","text":"<pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models etm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>ETM uses Word2Vec embeddings (300 dimensions).</p>"},{"location":"user-guide/training/#ctm-training","title":"CTM Training","text":"<pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models ctm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>CTM uses SBERT embeddings (768 dimensions).</p>"},{"location":"user-guide/training/#dtm-training","title":"DTM Training","text":"<pre><code>python run_pipeline.py \\\n    --dataset temporal_dataset \\\n    --models dtm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --hidden_dim 512 \\\n    --learning_rate 0.002 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>DTM models topic evolution across time slices defined by the time column in preprocessing.</p>"},{"location":"user-guide/training/#training-multiple-models","title":"Training Multiple Models","text":"<p>Compare multiple models simultaneously:</p> <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models lda,etm,ctm \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>Models train sequentially. Results are saved in separate directories for comparison.</p>"},{"location":"user-guide/visualization/","title":"Visualization","text":"<p>Training automatically generates visualizations. Additional visualizations can be created separately.</p>"},{"location":"user-guide/visualization/#visualization-outputs","title":"Visualization Outputs","text":"<p>topic_words_bars.png Bar charts showing top-10 words for each topic with probability weights.</p> <p>topic_similarity.png Heatmap showing cosine similarity between topic-word distributions.</p> <p>doc_topic_umap.png UMAP projection of documents in topic space. Points are colored by dominant topic.</p> <p>topic_wordclouds.png Word clouds for each topic sized by word probability.</p> <p>metrics.png Bar charts comparing evaluation metrics.</p> <p>pyldavis.html Interactive visualization using pyLDAvis library. Open in web browser.</p>"},{"location":"user-guide/visualization/#generating-visualizations-separately","title":"Generating Visualizations Separately","text":"<p>Generate visualizations for THETA models:</p> <pre><code>cd /root/autodl-tmp/ETM\n\npython -m visualization.run_visualization \\\n    --result_dir /root/autodl-tmp/result/0.6B \\\n    --dataset my_dataset \\\n    --mode zero_shot \\\n    --model_size 0.6B \\\n    --language en \\\n    --dpi 300\n</code></pre> <p>Generate visualizations for baseline models:</p> <pre><code>python -m visualization.run_visualization \\\n    --baseline \\\n    --result_dir /root/autodl-tmp/result/baseline \\\n    --dataset my_dataset \\\n    --model lda \\\n    --num_topics 20 \\\n    --language en \\\n    --dpi 300\n</code></pre> <p>Replace <code>lda</code> with <code>etm</code>, <code>ctm</code>, or <code>dtm</code> for other baseline models.</p>"},{"location":"user-guide/visualization/#customizing-visualization","title":"Customizing Visualization","text":"<p>Higher resolution: <pre><code>python -m visualization.run_visualization \\\n    --result_dir /root/autodl-tmp/result/0.6B \\\n    --dataset my_dataset \\\n    --mode zero_shot \\\n    --model_size 0.6B \\\n    --language en \\\n    --dpi 600\n</code></pre></p> <p>Chinese language visualizations: <pre><code>python -m visualization.run_visualization \\\n    --result_dir /root/autodl-tmp/result/0.6B \\\n    --dataset chinese_dataset \\\n    --mode zero_shot \\\n    --model_size 0.6B \\\n    --language zh \\\n    --dpi 300\n</code></pre></p> <p>Chinese visualizations use appropriate fonts and handle character rendering correctly.</p>"},{"location":"user-guide/visualization/#skipping-visualization-during-training","title":"Skipping Visualization During Training","text":"<p>Skip automatic visualization to save time:</p> <pre><code>python run_pipeline.py \\\n    --dataset my_dataset \\\n    --models theta \\\n    --model_size 0.6B \\\n    --mode zero_shot \\\n    --num_topics 20 \\\n    --epochs 100 \\\n    --batch_size 64 \\\n    --skip-viz \\\n    --gpu 0 \\\n    --language en\n</code></pre> <p>Visualizations can be generated later using the separate visualization command.</p>"}]}